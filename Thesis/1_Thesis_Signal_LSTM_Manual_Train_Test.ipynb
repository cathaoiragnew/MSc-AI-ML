{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_July_Signal_Manual_Train_Test_Cathaoir_V2_Thesis_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN84-i0Cqz52"
      },
      "source": [
        "# Signal Measurements\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSZWa4RAQsNg"
      },
      "source": [
        "# Imports\n",
        "\n",
        "\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AhOxn2FQsAw"
      },
      "source": [
        "\n",
        "########################### (https://github.com/curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs/blob/master/human_activity_recognition.ipynb) imports\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "RANDOM_SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_M5F5DwPrKk"
      },
      "source": [
        "# Upload Data \n",
        "\n",
        "---------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gfe3HF-vVBm"
      },
      "source": [
        "## Signal "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZX9OGsWPzvD"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IyDYp-eLY8K"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_train = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXPVt1AP0v1"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDlYInx_Pyx5"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWh-0CdrvGmy"
      },
      "source": [
        "\n",
        "## Labels "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI6mZQZpP4fg"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo_VGwhJvG5z"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_label_train = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWQaWpwJP5iK"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ii4LZrUP8hS"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_label_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIL8q5nvqUf"
      },
      "source": [
        "### Reading in segments & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y70o5U5mSOUU"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxXU40eBttE6"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_signals_train = [key for key in uploaded_signal_train.keys()]\n",
        "list_of_labels_train  = [key for key in uploaded_label_train.keys()]\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_signal_list_train = [] \n",
        "all_label_list_train = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_signals_train)):\n",
        "\n",
        "  # check to make sure ID's match for signal & label, first two elements of file name will be ID as this is the naming convention choosen \n",
        "  if list_of_signals_train[i][:2] == list_of_labels_train[i][:2]:\n",
        "\n",
        "    # load in the data \n",
        "    signal_train = np.load(list_of_signals_train[i])\n",
        "    label_train  = np.load(list_of_labels_train[i])\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_signal_list_train.append(signal_train) \n",
        "    all_label_list_train.append(label_train) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXzYX7vVSPqK"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s9Wo8YGSQeN"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_signals_test = [key for key in uploaded_signal_test.keys()]\n",
        "list_of_labels_test  = [key for key in uploaded_label_test.keys()]\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_signal_list_test = [] \n",
        "all_label_list_test = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_signals_test)):\n",
        "\n",
        "  # check to make sure ID's match for signal & label, first two elements of file name will be ID as this is the naming convention choosen \n",
        "  if list_of_signals_test[i][:2] == list_of_labels_test[i][:2]:\n",
        "\n",
        "    # load in the data \n",
        "    signal_test = np.load(list_of_signals_test[i])\n",
        "    label_test  = np.load(list_of_labels_test[i])\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_signal_list_test.append(signal_test) \n",
        "    all_label_list_test.append(label_test) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD1s9JxEw7UF"
      },
      "source": [
        "### Concate together for both signals and labels "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcTdMphSl-C"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfHfNYc_uVDm"
      },
      "source": [
        "# concate the signal and labels\n",
        "all_signal_concat_train = np.concatenate((all_signal_list_train))\n",
        "all_label_concat_train = np.concatenate((all_label_list_train))\n",
        "\n",
        "# unique values\n",
        "print(np.unique(all_label_concat_train, return_counts=True))\n",
        "\n",
        "# print out shapes as these should be equal\n",
        "print(f\"\\nSignal Shape {all_signal_concat_train.shape}\")\n",
        "print(f\"Label Shape {all_label_concat_train.shape}\")\n",
        "\n",
        "#finally convert labels to dummy variables as this is more favourable in deep learning \n",
        "\n",
        "# convert to dummy category\n",
        "all_label_concat_train_dum = pd.get_dummies(all_label_concat_train)\n",
        "\n",
        "print(f'\\nDummy labels Shape {all_label_concat_train_dum.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xBN_7VvmBpr"
      },
      "source": [
        "'not set' in np.unique(all_label_concat_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK1UqZpbSoRQ"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sGbKpc2S7um"
      },
      "source": [
        "# concate the signal and labels\n",
        "all_signal_concat_test = np.concatenate((all_signal_list_test))\n",
        "all_label_concat_test = np.concatenate((all_label_list_test))\n",
        "\n",
        "# unique values\n",
        "print(np.unique(all_label_concat_test, return_counts=True))\n",
        "\n",
        "# print out shapes as these should be equal\n",
        "print(f\"\\nSignal Shape {all_signal_concat_test.shape}\")\n",
        "print(f\"Label Shape {all_label_concat_test.shape}\")\n",
        "\n",
        "#finally convert labels to dummy variables as this is more favourable in deep learning \n",
        "\n",
        "# convert to dummy category\n",
        "all_label_concat_test_dum = pd.get_dummies(all_label_concat_test)\n",
        "\n",
        "print(f'\\nDummy labels Shape {all_label_concat_test_dum.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQnmoCYwVMxv"
      },
      "source": [
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKp629ysdVMc"
      },
      "source": [
        "# for i in list_of_signals_train[:10]:\n",
        "\n",
        "#   if i in list_of_signals_train[:10]:\n",
        "\n",
        "#     print(\"yes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsTiithFVNqD"
      },
      "source": [
        "#Check Train/Test no overlap\n",
        "\n",
        "\n",
        "for i in list_of_signals_train:\n",
        "\n",
        "  if i in list_of_signals_test:\n",
        "    print(\"Leak\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X7OgdfwIHcy"
      },
      "source": [
        "# DataFrame to balance out labels \n",
        "\n",
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_C-aIjAIGvB"
      },
      "source": [
        "# df = pd.DataFrame()\n",
        "\n",
        "# df['Label'] = all_label_concat\n",
        "\n",
        "# df['Index_num'] = df.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFeIjqySMe7W"
      },
      "source": [
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qUCUEfUL4_j"
      },
      "source": [
        "# val_counts = df['Label'].value_counts()\n",
        "# min_val_count = val_counts.min()\n",
        "\n",
        "# new_df = df.groupby('Label').apply(lambda x: x.sample(min_val_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRPkIAdaMJgA"
      },
      "source": [
        "# new_df['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-GF-rpRMPy4"
      },
      "source": [
        "# mask = new_df['Index_num'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq40C1IWMxm8"
      },
      "source": [
        "# ss = all_signal_concat[mask]\n",
        "\n",
        "# ls = all_label_concat[mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N168dn6gYDh7"
      },
      "source": [
        "# # convert to dummy category\n",
        "# ls_dum = pd.get_dummies(ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqIMMXsQR9ks"
      },
      "source": [
        "# Split Train Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA6xJ_Z2lg_0"
      },
      "source": [
        "# All Features 3 Axis 3 Features Accel , Mag , Gyro \n",
        "\n",
        "all_signal_concat_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iqtuCJHllZl"
      },
      "source": [
        "# This is only Accel, features are in following shape  -  X Acc  Y Acc  Z Acc  X Mag  Y Mag Z Mag  X Gyro  Y Gyro Z Gyro\n",
        "\n",
        "all_signal_concat_train[:,:,0:3].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRnTisaSR_kp"
      },
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(all_signal_concat, all_label_concat_dum, test_size=0.25, random_state=RANDOM_SEED , stratify = all_label_concat_dum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huBp_r8dNjv7"
      },
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(ss, ls_dum, test_size=0.25, random_state=RANDOM_SEED , stratify = ls_dum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4a4RW6JTmSk"
      },
      "source": [
        "X_train , X_test = all_signal_concat_train , all_signal_concat_test\n",
        "y_train , y_test = all_label_concat_train_dum , all_label_concat_test_dum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3D7jMKvSMlZ"
      },
      "source": [
        "# print out length \n",
        "print(\"Train Size:\" ,len(X_train))\n",
        "print(\"Test Size:\" ,len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srrQneAMjZSC"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXdqd1PDXeE6"
      },
      "source": [
        "-----------------------------\n",
        "\n",
        "Plain LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXNvrnDQaTDp"
      },
      "source": [
        "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I0C1WLmaWXo"
      },
      "source": [
        "# lstm model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFUCbu4-Oe7"
      },
      "source": [
        "-------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRfPtPhbBhVi"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoErRxn6IeOi"
      },
      "source": [
        "import time\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_GdiDklvlJU"
      },
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[1] , X_train.shape[2] , y_train.shape[1]\n",
        "\n",
        "# set up model\n",
        "model = Sequential()\n",
        "\n",
        "#input layer\n",
        "model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "# hidden layers\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(n_outputs, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csOeYlKAQbgY"
      },
      "source": [
        "# n_timesteps, n_features, n_outputs = X_train.shape[1] , X_train.shape[2] , y_train.shape[1]\n",
        "\n",
        "# # set up model\n",
        "# model = Sequential()\n",
        "\n",
        "# #input layer\n",
        "# model.add(LSTM(100,return_sequences= True , input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "# model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True , input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "# model.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.35, return_sequences=False, input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "\n",
        "# # output layer\n",
        "# model.add(Dense(n_outputs, activation='softmax'))\n",
        "\n",
        "# # compile model\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5z61aTZyo9b"
      },
      "source": [
        "----------------------\n",
        "\n",
        "https://stackoverflow.com/questions/40496069/reset-weights-in-keras-layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoL55nL-yYsX"
      },
      "source": [
        "# save initial weights, so can reload to base weights in later hacker CV\n",
        "model.save_weights('base_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnYjLuZEDW62"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N1IdVSZrV9X"
      },
      "source": [
        "Optimize \n",
        "\n",
        "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBB_TbWa9sIl"
      },
      "source": [
        "https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mjd3PWGgdVS"
      },
      "source": [
        "https://stackoverflow.com/questions/48285129/saving-best-model-in-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiyl3ohOEvQW"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        " \t\n",
        "# patient early stopping\n",
        "#es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=5 , min_delta=0.05)\n",
        "\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint('.base_model_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLEy9dDL1g41"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVkSAFQE8A2c"
      },
      "source": [
        "# just for timing model\n",
        "t0 = time.time()\n",
        "\n",
        "# fitting model \n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs = 100,\n",
        "                    batch_size=200,\n",
        "                    callbacks=[earlyStopping, mcp_save]\n",
        "                    )\n",
        "\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "total = t1-t0\n",
        "\n",
        "print(f'Time Taken: {total}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lTZXCf1g9t_"
      },
      "source": [
        "# model.save_weights(filepath = '.base_model_wts.hdf5')\n",
        "\n",
        "# files.download('.base_model_wts.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N5dd-Q01i4A"
      },
      "source": [
        "----------------------------\n",
        "\n",
        "# Learning Curves\n",
        "\n",
        "https://vitalflux.com/python-keras-learning-validation-curve-classification-model/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3NGPoQL1pU8"
      },
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "accuracy = history_dict['categorical_accuracy']\n",
        "val_accuracy = history_dict['val_categorical_accuracy']\n",
        " \n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "#\n",
        "# Plot the model accuracy vs Epochs\n",
        "#\n",
        "ax[0].plot(epochs, accuracy, 'r', label='Training accuracy')\n",
        "ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
        "ax[0].set_xlabel('Epochs', fontsize=16)\n",
        "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
        "ax[0].legend()\n",
        "#\n",
        "# Plot the loss vs Epochs\n",
        "#\n",
        "ax[1].plot(epochs, loss_values, 'r', label='Training loss')\n",
        "ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
        "ax[1].set_xlabel('Epochs', fontsize=16)\n",
        "ax[1].set_ylabel('Loss', fontsize=16)\n",
        "ax[1].legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-9luGZA6S4o"
      },
      "source": [
        "-----------\n",
        "# Classification Reports "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI5PTbCb6MJW"
      },
      "source": [
        "Make Predicitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pZrUb3e9SSH"
      },
      "source": [
        "# For Confusion matrix & Classification Report \n",
        "\n",
        "y_preds = model.predict(X_test)\n",
        "max_predictions = np.argmax(y_preds, axis=1)\n",
        "\n",
        "max_test = np.argmax(np.asarray(y_test), axis=1)\n",
        "\n",
        "##################\n",
        "\n",
        "\n",
        "y_preds_t = model.predict(X_train)\n",
        "\n",
        "max_predictions_t = np.argmax(y_preds_t, axis=1)\n",
        "max_train = np.argmax(np.asarray(y_train), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocbl1VTVaZXO"
      },
      "source": [
        "# Metrics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGPfaIQna1qB"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63JnyIrJaai2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score, precision_score , recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"Test Accuracy : \\t\\t\" ,accuracy_score(max_test, max_predictions))\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(max_test, max_predictions))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(max_test, max_predictions , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(max_test, max_predictions , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(max_test, max_predictions , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EjHZjy0a2vF"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6DnA8cfa_wz"
      },
      "source": [
        "print(\"Train Accuracy : \\t\\t\" ,accuracy_score(max_train, max_predictions_t))\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(max_train, max_predictions_t))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(max_train, max_predictions_t, average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(max_train, max_predictions_t , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(max_train, max_predictions_t , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2LpL8jl3WE1"
      },
      "source": [
        "Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9cJEhcm7Hcv"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# set up labels \n",
        "LABELS = ['Go', 'Turn1',  'Turn2' , 'Walk1', 'Walk2', 'Sit']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VPW38Ol13r7"
      },
      "source": [
        "# Test Set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmqio-IFxVKT"
      },
      "source": [
        "print(classification_report(max_test, max_predictions , target_names=LABELS))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmAM0Jp115kv"
      },
      "source": [
        "# Train Set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLpbBnmV11_2"
      },
      "source": [
        "print(classification_report(max_train, max_predictions_t , target_names=LABELS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGyuwOAC3RTT"
      },
      "source": [
        "--------------------\n",
        "\n",
        "# Confusion Matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3I2gx2Z_BGG"
      },
      "source": [
        "confusion_matrix = metrics.confusion_matrix(max_test, max_predictions )\n",
        "\n",
        "confusion_matrix_t = metrics.confusion_matrix(max_train, max_predictions_t )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU1i78_3_m23"
      },
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True , fmt=\"d\");\n",
        "plt.title(\"Test Data Confusion matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u31mjLP2Dxwp"
      },
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(confusion_matrix_t, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Train Confusion matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll9CM1mBzBl6"
      },
      "source": [
        "----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3lPlN2Ypxkh"
      },
      "source": [
        "# import random\n",
        "\n",
        "\n",
        "# random.seed(4)\n",
        "# rands = random.sample(range(100), 3)\n",
        "\n",
        "# # hacker man CV - using train test split with different seeds\n",
        "\n",
        "# # just a counter\n",
        "# run_num = 0 \n",
        "\n",
        "# for random_num in rands: \n",
        "\n",
        "#   print(f'\\n----------------------- Run: {run_num} ---------------------------------------------')\n",
        "\n",
        "#   # reset base weights to prevent any leak between train & test\n",
        "#   model.load_weights('base_model.h5')\n",
        "\n",
        "#   # this will give different splits each time since different random seed \n",
        "#   X_train, X_test, y_train, y_test = train_test_split(all_signal_concat, all_label_concat_dum, test_size=0.25, random_state= random_num , stratify = all_label_concat_dum)\n",
        "\n",
        "   \t\n",
        "#   # patient early stopping\n",
        "#   es = EarlyStopping(monitor='val_accuracy', mode='auto', verbose=1, patience=5 , min_delta=0.1)\n",
        "\n",
        "#   ### Train Model ###\n",
        "\n",
        "#   # just for timing model\n",
        "#   t0 = time.time()\n",
        "\n",
        "#   # fitting model \n",
        "#   history = model.fit(X_train, y_train,\n",
        "#                       validation_data=(X_test, y_test),\n",
        "#                       epochs=30,\n",
        "#                       batch_size=20, verbose = 0\n",
        "#                       )\n",
        "  \n",
        "#   t1 = time.time()\n",
        "#   total = t1-t0\n",
        "#   print(f'Time Taken: {total}')\n",
        "\n",
        "#   ########################## Output Metrics ############################################\n",
        "\n",
        "#   # Learning Curves \n",
        "#   history_dict = history.history\n",
        "#   loss_values = history_dict['loss']\n",
        "#   val_loss_values = history_dict['val_loss']\n",
        "#   accuracy = history_dict['accuracy']\n",
        "#   val_accuracy = history_dict['val_accuracy']\n",
        "  \n",
        "#   epochs = range(1, len(loss_values) + 1)\n",
        "#   fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "#   #\n",
        "#   # Plot the model accuracy vs Epochs\n",
        "#   #\n",
        "#   ax[0].plot(epochs, accuracy, 'r', label='Training accuracy')\n",
        "#   ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "#   ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
        "#   ax[0].set_xlabel('Epochs', fontsize=16)\n",
        "#   ax[0].set_ylabel('Accuracy', fontsize=16)\n",
        "#   ax[0].legend()\n",
        "#   #\n",
        "#   # Plot the loss vs Epochs\n",
        "#   #\n",
        "#   ax[1].plot(epochs, loss_values, 'r', label='Training loss')\n",
        "#   ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "#   ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
        "#   ax[1].set_xlabel('Epochs', fontsize=16)\n",
        "#   ax[1].set_ylabel('Loss', fontsize=16)\n",
        "#   ax[1].legend()\n",
        "\n",
        "\n",
        "#   # For Confusion matrix & Classification Report \n",
        "\n",
        "#   # predict test data \n",
        "\n",
        "#   y_preds = model.predict(X_test)\n",
        "#   max_predictions = np.argmax(y_preds, axis=1)\n",
        "\n",
        "#   max_test = np.argmax(np.asarray(y_test), axis=1)\n",
        "\n",
        "#   ##################\n",
        "\n",
        "#   # predict train data \n",
        "#   y_preds_t = model.predict(X_train)\n",
        "\n",
        "#   max_predictions_t = np.argmax(y_preds_t, axis=1)\n",
        "#   max_train = np.argmax(np.asarray(y_train), axis=1)\n",
        "\n",
        "\n",
        "#   print(\"\\nTest Data Report\\n\")\n",
        "#   print(classification_report(max_test, max_predictions , target_names=LABELS))\n",
        "\n",
        "#   print(\"\\nTrain Data Report\\n\")\n",
        "#   print(classification_report(max_train, max_predictions_t , target_names=LABELS))\n",
        "\n",
        "\n",
        "\n",
        "#   # user input to interrupt each run so can look \n",
        "\n",
        "#   #val = input(\"Press Anything to proceed: \")\n",
        "\n",
        "\n",
        "#   # increment run_num\n",
        "#   run_num += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxiYNoIxMVb"
      },
      "source": [
        "------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF0sQzFHnl_g"
      },
      "source": [
        "# Read in Fresh Data to see what predictions looks like\n",
        "from google.colab import files\n",
        "uploaded_signal_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyuGjt3Anpzu"
      },
      "source": [
        "signal_test = np.load('34_3_Participant_SlideSize_10_Signal_Segments_RJP.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqpWZYklVPvN"
      },
      "source": [
        "# Read in Fresh Data to see what predictions looks like\n",
        "from google.colab import files\n",
        "uploaded_label_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOaj69eGVSUf"
      },
      "source": [
        "signal_label = np.load('34_3_Participant_SlideSize_10_Labels_Segments_RJP.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS9WRR8VVYhT"
      },
      "source": [
        "yy_test = pd.get_dummies(signal_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTNNi2MTVo0A"
      },
      "source": [
        "yy_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51q706jVm5lu"
      },
      "source": [
        "# Sliding window = 10 ->   10 points that are 0.01 seconds each -> this segment covers ( 10 * 0.01 ) = 0.1  seconds   since 50% overlap actually will be midpoints\n",
        "\n",
        "# 0    - 0.1      (label 1)\n",
        "# 0.5  - 1.5      (label 2)\n",
        "# 0.1  - 0.2      (label 3)\n",
        "# 0.15 - 0.25     (label 4)\n",
        "\n",
        "# so use midpoints\n",
        "\n",
        "# 0    - 0.05     (label 1)\n",
        "# 0.05 - 0.1      (label 2)\n",
        "# 0.1  - 0.15     (label 3)\n",
        "# 0.15 - 0.2      (label 4)\n",
        "\n",
        "\n",
        "\n",
        "# which looks like \n",
        "\n",
        "# 0.05 - (label 1)\n",
        "# 0.1  - (label 2)\n",
        "# 0.15 - (label 3)\n",
        "# 0.2  - (label 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlffeAAvXwnI"
      },
      "source": [
        "signal_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha2yhzKxXzNR"
      },
      "source": [
        "yy_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzEzvwjvo_9p"
      },
      "source": [
        "y_preds = model.predict(signal_test)\n",
        "max_predictions = np.argmax(y_preds, axis=1)\n",
        "\n",
        "#\n",
        "max_testtt = np.argmax(np.asarray(yy_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAy6o85uVt6j"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "print(\"85 5 Cali Run Test Accuracy :\\t\\t\" ,accuracy_score(max_testtt, max_predictions))\n",
        "print(\"85 5 Cali Run Test Balanced Accuracy :\\t\" , balanced_accuracy_score(max_testtt, max_predictions))\n",
        "\n",
        "print(\"85 5 Cali Run Test F1 Score :\\t\\t\" , f1_score(max_testtt, max_predictions , average='weighted'))\n",
        "print(\"85 5 Cali Run Test Precision Score :\\t\" , precision_score(max_testtt, max_predictions, average='weighted'))\n",
        "print(\"85 5 Cali Run Test Recall Score :\\t\" , recall_score(max_testtt, max_predictions , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-P6R-j8UDMD"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qd1Trt8UFyX"
      },
      "source": [
        "confusion_matrix_out = metrics.confusion_matrix(max_testtt, max_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yXURCW1UH7E"
      },
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(confusion_matrix_out, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Test Data Confusion matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOEDYHG9gg3P"
      },
      "source": [
        "print(classification_report(max_testtt, max_predictions , target_names=LABELS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Okufmm8s8nM"
      },
      "source": [
        "print(y_preds[0])\n",
        "\n",
        "\n",
        "max_predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVlnOEBFnEWq"
      },
      "source": [
        "max_predictions[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX_HPnTjQkf8"
      },
      "source": [
        "--------------------\n",
        "\n",
        "Hyper Tuning \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G01FtvYIQ8nT"
      },
      "source": [
        "from keras.optimizers import Adam , RMSprop, Adagrad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaBrN0BQQ6Uc"
      },
      "source": [
        "optimisers = ['Adam' , 'RMSprop', 'Adagrad']\n",
        "\n",
        "batch_sizes = [50,200,1000]\n",
        "\n",
        "learning_rate = [0.001 , 0.01 ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQtR2PClQ2eV"
      },
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[1] , X_train.shape[2] , y_train.shape[1]\n",
        "\n",
        "# set up model\n",
        "model = Sequential()\n",
        "\n",
        "#input layer\n",
        "model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "# hidden layers\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(n_outputs, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp36iiDQ2eZ"
      },
      "source": [
        "Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrfOeaGDQ2ea"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        " \t\n",
        "# patient early stopping\n",
        "#es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=5 , min_delta=0.05)\n",
        "\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint('.base_model_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN4neLfj_o_K"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score , precision_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yo9tfQkQ2ed"
      },
      "source": [
        "Fit "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVnWue1LQ2ef"
      },
      "source": [
        "list_of_f1           = []\n",
        "list_of_recall       = []\n",
        "list_of_precision    = []\n",
        "list_of_balacc       = []\n",
        "list_of_learningrate = []\n",
        "list_of_optim        = []\n",
        "list_of_batch        = []\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(optimisers)):\n",
        "\n",
        "  for size in batch_sizes:\n",
        "\n",
        "    for learn in learning_rate:\n",
        "\n",
        "      # resets the weights back to random\n",
        "      model.load_weights('base_model.h5')\n",
        "\n",
        "      # compile model\n",
        "      model.compile(loss='categorical_crossentropy', optimizer= optimisers[i] , metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "      # set learning rate\n",
        "      K.set_value(model.optimizer.learning_rate, learn)\n",
        "\n",
        "      # just for timing model\n",
        "      t0 = time.time()\n",
        "\n",
        "      # fitting model \n",
        "      history = model.fit(X_train, y_train,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          epochs = 100,\n",
        "                          batch_size=size,\n",
        "                          callbacks=[earlyStopping, mcp_save]\n",
        "                          )\n",
        "\n",
        "\n",
        "      t1 = time.time()\n",
        "\n",
        "      total = t1-t0\n",
        "\n",
        "      print(f'Time Taken: {total}')\n",
        "\n",
        "      # For Confusion matrix & Classification Report \n",
        "\n",
        "      y_preds = model.predict(X_test)\n",
        "      max_predictions = np.argmax(y_preds, axis=1)\n",
        "\n",
        "      max_test = np.argmax(np.asarray(y_test), axis=1)\n",
        "\n",
        "      ##################\n",
        "\n",
        "\n",
        "      y_preds_train = model.predict(X_train)\n",
        "\n",
        "      max_predictions_train = np.argmax(y_preds_train, axis=1)\n",
        "      max_train = np.argmax(np.asarray(y_train), axis=1)\n",
        "\n",
        "\n",
        "      #############################\n",
        "\n",
        "      list_of_f1.append(f1_score(max_test, max_predictions , average='weighted'))          \n",
        "      list_of_recall.append(recall_score(max_test, max_predictions , average='weighted'))       \n",
        "      list_of_precision.append(precision_score(max_test, max_predictions , average='weighted'))    \n",
        "      list_of_balacc.append(balanced_accuracy_score(max_test, max_predictions))       \n",
        "      list_of_learningrate.append(learn) \n",
        "      list_of_optim.append(optimisers[i])\n",
        "      list_of_batch.append(size)\n",
        "\n",
        "\n",
        "      print(\"-\"*50)\n",
        "      print(\"\\n\")\n",
        "      #Metrics\n",
        "      print(f\"{optimisers[i]} - Learning Rate {learn} - Batch Size: {size}  -  Test Balanced Accuracy : \\t\" , balanced_accuracy_score(max_test, max_predictions))\n",
        "      print(f\"{optimisers[i]} - Learning Rate {learn} - Batch Size: {size}  -  Test F1 Score : \\t\\t\" , f1_score(max_test, max_predictions , average='weighted'))\n",
        "      print(f\"{optimisers[i]} - Learning Rate {learn} - Batch Size: {size}  -  Test Precision Score : \\t\\t\" , precision_score(max_test, max_predictions , average='weighted'))\n",
        "      print(f\"{optimisers[i]} - Learning Rate {learn} - Batch Size: {size}  -  Test Recall Score : \\t\\t\" , recall_score(max_test, max_predictions , average='weighted'))\n",
        "\n",
        "      print(\"\\n\")\n",
        "      print(\"-\"*50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHVQksoIN565"
      },
      "source": [
        "new_df = pd.DataFrame()\n",
        "\n",
        "new_df['Optimizer'] = list_of_optim\n",
        "new_df['BatchSize'] = list_of_batch\n",
        "new_df['LearningRate'] = list_of_learningrate\n",
        "new_df['F1-Score'] = list_of_f1\n",
        "new_df['Balanced Accuracy'] = list_of_balacc\n",
        "new_df['Recall Score'] = list_of_recall\n",
        "new_df['Precision Score'] = list_of_precision\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6ITyluiCDQ5"
      },
      "source": [
        "display(new_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0JuzqeuORAM"
      },
      "source": [
        "new_df[ (new_df['LearningRate'] == 0.001)  | (new_df['LearningRate'] == 0.01)   ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9tqBEx0S_h6"
      },
      "source": [
        "-------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L38p1vViz9g"
      },
      "source": [
        "# so create 0.05 steps for length of entire array, to be used as time index \n",
        "\n",
        "time_steps = [i for i in np.arange(0.05, len(max_predictions)/(10*2) + 0.05 , 0.05 )]\n",
        "\n",
        "print(len(time_steps))\n",
        "\n",
        "print(len(max_predictions))\n",
        "print(len(y_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOaP54MGk0Q7"
      },
      "source": [
        "time_test  =  pd.to_datetime(time_steps, unit='s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqMTvs_2m2k2"
      },
      "source": [
        "time_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrggBa4yoxoK"
      },
      "source": [
        "# set up df\n",
        "predictions_dataframe = pd.DataFrame()\n",
        "\n",
        "# Add time as index \n",
        "predictions_dataframe['Preds'] = max_predictions\n",
        "predictions_dataframe['Time']  = time_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3IzZIyw94Cl"
      },
      "source": [
        "predictions_dataframe.Preds.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTu59GsUq3iJ"
      },
      "source": [
        "# look at 0 \"Go\" Predictions\n",
        "predictions_dataframe[predictions_dataframe['Preds'] == 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sch2Vnlx9uJ1"
      },
      "source": [
        "# look at 1 \"Turn 1\" Predictions\n",
        "predictions_dataframe[predictions_dataframe['Preds'] == 1 ].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwMyCc3Q-dUG"
      },
      "source": [
        "# look at 2 , \"Turn 2\" Predictions\n",
        "predictions_dataframe[predictions_dataframe['Preds'] == 2 ].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiJZI02o-goN"
      },
      "source": [
        "# look at 3 , \"Walk 1\" Predictions\n",
        "predictions_dataframe[predictions_dataframe['Preds'] == 3 ].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tQUPWF69xlq"
      },
      "source": [
        "# look at 4 , \"Walk 2\" Predictions\n",
        "predictions_dataframe[predictions_dataframe['Preds'] == 4 ].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk_n6uKE-txF"
      },
      "source": [
        "# look at 5 , \"Sit\" Predictions\n",
        "predictions_dataframe[predictions_dataframe['Preds'] == 5 ].head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D2-H07Hr-1l"
      },
      "source": [
        "print(predictions_dataframe.head(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npl-RMf8aO0B"
      },
      "source": [
        "---------------------------------------------\n",
        "\n",
        "Attempt to Classify stages \n",
        "\n",
        "- First should always be Go, so this can be hard coded\n",
        "- Last should always be Sit, so this can be hard coded\n",
        "\n",
        "\n",
        "- Going to want to find 4 out of 5/6 in a row as first detection  \n",
        "\n",
        "- Want to find the maximum length of index ID in a row that have same label - this can tell us we are still in the same stage and to filter for next stage after this last index \n",
        "\n",
        "\n",
        "- Once found approx max list indexs,  condition that next longest ID has to big bigger than that index,  as its sequential Go -> Walk 1 -> Turn 1 -> Walk 2 -> Turn 2 -> Sit \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTVfkWbuaRfI"
      },
      "source": [
        "# Hard Code\n",
        "\n",
        "# Go at beginnning\n",
        "predictions_dataframe.at[0, 'Preds'] = 0\n",
        "\n",
        "# Sit at end ( -1 because of 0 indexing)\n",
        "predictions_dataframe.at[ len(predictions_dataframe)-1 , 'Preds'] = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYTc_iOpDMMr"
      },
      "source": [
        "\n",
        "----------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cqTeGQcDPEZ"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQqgPB9bii81"
      },
      "source": [
        "\n",
        "\n",
        "Longest Running \n",
        "\n",
        "https://stackoverflow.com/questions/55861000/find-longest-consecutive-sub-array-not-sorted-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvnzhgLvmyBW"
      },
      "source": [
        "# Go \n",
        "\n",
        "----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bja0zyARmNn2"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 0 ].index:\n",
        "    if candidate and candidate[-1] != i - 1:\n",
        "        if len(candidate) > len(longest):\n",
        "            longest = candidate\n",
        "        candidate = []\n",
        "    candidate.append(i)\n",
        "if len(candidate) > len(longest):\n",
        "    longest = candidate\n",
        "\n",
        "\n",
        "longest_go = longest\n",
        "\n",
        "print(longest_go)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JTqf2UKly_G"
      },
      "source": [
        "# Walk 1 \n",
        "\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS7oaZf1Dimc"
      },
      "source": [
        "**First Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9psiAPyuDb5J"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "\n",
        "# for each index value in the sliced dataframe\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 3 ].index:\n",
        "\n",
        "\n",
        "    # if statement so it doesnt go backwards into already classified indexs\n",
        "    #if i > longest_go[-1]:\n",
        "    if i > 0:\n",
        "   \n",
        "\n",
        "      # check the difference between the index, if there are not zero\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) >= 3:\n",
        "\n",
        "            # only interested in first one, so when we get it break for loop\n",
        "            first = candidate\n",
        "            break\n",
        "\n",
        "          # clear candidate and start again\n",
        "          candidate = []\n",
        "\n",
        "      # if avoids if statements above means its a consectitive index so just append    \n",
        "      candidate.append(i)\n",
        "\n",
        "first_walk_1 = first\n",
        "\n",
        "# where on interested in first 3 in a row\n",
        "print(first_walk_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDCHB3iEDcQg"
      },
      "source": [
        "**Longest in a row**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8RDufkPl06U"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "print(longest_go[-1])\n",
        "\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 3 ].index:\n",
        "\n",
        "    # if statement so it doesnt go backwards into already classified indexs\n",
        "    #if i > longest_go[-1]:\n",
        "    if i > 0:\n",
        "\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) > len(longest):\n",
        "              longest = candidate\n",
        "          candidate = []\n",
        "      candidate.append(i)\n",
        "\n",
        "\n",
        "if len(candidate) > len(longest):\n",
        "    longest = candidate\n",
        "\n",
        "\n",
        "longest_walk_1 = longest\n",
        "\n",
        "print(longest_walk_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL1smEpjjyIn"
      },
      "source": [
        "# Turn 1 \n",
        "\n",
        "------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6lT4iMHJQsi"
      },
      "source": [
        "**First Dectection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mUjasWDJSxA"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "\n",
        "# for each index value in the sliced dataframe\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 1 ].index:\n",
        "\n",
        "\n",
        "    # if statement so it doesnt go backwards into already classified indexs\n",
        "    if i > longest_walk_1[-1]:\n",
        "\n",
        "      # check the difference between the index, if there are not zero means no longer consecutive\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) >= 3:\n",
        "\n",
        "            # only interested in first one, so when we get it break for loop\n",
        "            first = candidate\n",
        "            break\n",
        "\n",
        "          # clear candidate and start again\n",
        "          candidate = []\n",
        "\n",
        "      # if avoids if statements above means its a consectitive index so just append    \n",
        "      candidate.append(i)\n",
        "\n",
        "first_turn_1 = first\n",
        "\n",
        "# where on interested in first 3 in a row\n",
        "print(first_turn_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzR964cSJTSy"
      },
      "source": [
        "**Longest in a row**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ZTHCo3i4H-"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 1 ].index:\n",
        "\n",
        "    if i > longest_walk_1[-1]:\n",
        "\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) > len(longest):\n",
        "              longest = candidate\n",
        "          candidate = []\n",
        "      candidate.append(i)\n",
        "      \n",
        "if len(candidate) > len(longest):\n",
        "    longest = candidate\n",
        "\n",
        "\n",
        "longest_turn_1 = longest\n",
        "\n",
        "print(longest_turn_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc9eqhGzKyd-"
      },
      "source": [
        "# Walk 2 \n",
        "\n",
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E16SJmLyK0sT"
      },
      "source": [
        "**First Dectection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZxZrSkK2ul"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "\n",
        "# for each index value in the sliced dataframe\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 4 ].index:\n",
        "\n",
        "\n",
        "    # if statement so it doesnt go backwards into already classified indexs\n",
        "    if i > longest_turn_1[-1]:\n",
        "\n",
        "      # check the difference between the index, if there are not zero means no longer consecutive\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) >= 3:\n",
        "\n",
        "            # only interested in first one, so when we get it break for loop\n",
        "            first = candidate\n",
        "            break\n",
        "\n",
        "          # clear candidate and start again\n",
        "          candidate = []\n",
        "\n",
        "      # if avoids if statements above means its a consectitive index so just append    \n",
        "      candidate.append(i)\n",
        "\n",
        "first_walk_2 = first\n",
        "\n",
        "# where on interested in first 3 in a row\n",
        "print(first_walk_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNvcDGBwLSGb"
      },
      "source": [
        "**Longest in a run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_b87-D7LUOA"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 4 ].index:\n",
        "\n",
        "    if i > longest_turn_1[-1]:\n",
        "\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) > len(longest):\n",
        "              longest = candidate\n",
        "          candidate = []\n",
        "      candidate.append(i)\n",
        "\n",
        "      \n",
        "if len(candidate) > len(longest):\n",
        "    longest = candidate\n",
        "\n",
        "\n",
        "longest_walk_2 = longest\n",
        "\n",
        "print(longest_walk_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aqiUOilkG-_"
      },
      "source": [
        "# Turn 2\n",
        "\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLwaqYfeKgJk"
      },
      "source": [
        "**First Dectection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV31vqdyKh4S"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "\n",
        "# for each index value in the sliced dataframe\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 2 ].index:\n",
        "\n",
        "\n",
        "    # if statement so it doesnt go backwards into already classified indexs\n",
        "    if i > longest_walk_2[-1]:\n",
        "    \n",
        "    \n",
        "      # check the difference between the index, if there are not zero means no longer consecutive\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) >= 3:\n",
        "\n",
        "            # only interested in first one, so when we get it break for loop\n",
        "            first = candidate\n",
        "            break\n",
        "\n",
        "          # clear candidate and start again\n",
        "          candidate = []\n",
        "\n",
        "      # if avoids if statements above means its a consectitive index so just append    \n",
        "      candidate.append(i)\n",
        "\n",
        "first_turn_2 = first\n",
        "\n",
        "# where on interested in first 3 in a row\n",
        "print(first_turn_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoidW2O9KojM"
      },
      "source": [
        "**Longest in a row**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv7Ih9qyjtp4"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 2 ].index:\n",
        "\n",
        "    if i <= longest_walk_2[-1]:\n",
        "      continue\n",
        "\n",
        "    if candidate and candidate[-1] != i - 1:\n",
        "        if len(candidate) > len(longest):\n",
        "            longest = candidate\n",
        "        candidate = []\n",
        "    candidate.append(i)\n",
        "if len(candidate) > len(longest):\n",
        "    longest = candidate\n",
        "\n",
        "\n",
        "longest_turn_2 = longest\n",
        "\n",
        "print(longest_turn_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDRcqMjwm_ZX"
      },
      "source": [
        "# Sit\n",
        "\n",
        "----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AilZ1UXMFQp"
      },
      "source": [
        "**First Dectection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7be5CVq-MH89"
      },
      "source": [
        "candidate = []\n",
        "longest = []\n",
        "\n",
        "\n",
        "# for each index value in the sliced dataframe\n",
        "for i in predictions_dataframe[predictions_dataframe['Preds'] == 5 ].index:\n",
        "\n",
        "\n",
        "    # if statement so it doesnt go backwards into already classified indexs\n",
        "    if i > longest_turn_2[-1]:\n",
        "\n",
        "      # check the difference between the index, if there are not zero means no longer consecutive\n",
        "      if candidate and candidate[-1] != i - 1:\n",
        "          if len(candidate) >= 3:\n",
        "\n",
        "            # only interested in first one, so when we get it break for loop\n",
        "            first = candidate\n",
        "            break\n",
        "\n",
        "          # clear candidate and start again\n",
        "          candidate = []\n",
        "\n",
        "      # if avoids if statements above means its a consectitive index so just append    \n",
        "      candidate.append(i)\n",
        "\n",
        "first_sit = first\n",
        "\n",
        "# where on interested in first 3 in a row\n",
        "print(first_sit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKJg5ppnMl0W"
      },
      "source": [
        "Dont need longest in a run as Sit is final phase + last entry is hard coded so can simply use that "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZjDusneMt1E"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUgStg8gMvCw"
      },
      "source": [
        "---------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVlfHZINMwhr"
      },
      "source": [
        "print(\"\\n First walk  : \", first_walk_1[0])\n",
        "print(\"\\n First turn 1: \", first_turn_1[0])\n",
        "print(\"\\n First walk 2: \", first_walk_2[0])\n",
        "print(\"\\n First turn 2: \", first_turn_2[0])\n",
        "print(\"\\n First Sit   : \", first_sit[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YVt7-deNn7q"
      },
      "source": [
        "# create first times dataframes\n",
        "\n",
        "go = predictions_dataframe.iloc[0]['Time']\n",
        "walk_1 = predictions_dataframe.iloc[first_walk_1[0]]['Time']\n",
        "turn_1 = predictions_dataframe.iloc[first_turn_1[0]]['Time']\n",
        "walk_2 = predictions_dataframe.iloc[first_walk_2[0]]['Time']\n",
        "turn_2 = predictions_dataframe.iloc[first_turn_2[0]]['Time']\n",
        "sit = predictions_dataframe.iloc[first_sit[0]]['Time']\n",
        "end = predictions_dataframe.iloc[-1]['Time']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJy3oUJjOcCf"
      },
      "source": [
        "\n",
        "print(\"\\nTime Difference: Walk 1 - Go   (Getting Up) :\", walk_1 - go)\n",
        "\n",
        "print(\"\\nTime Difference: Turn 1 - Walk 1 (Walking 1) :\", turn_1 - walk_1)\n",
        "\n",
        "print(\"\\nTime Difference: Walk 2 - Turn 1 (Turning 1) :\", walk_2 - turn_1)\n",
        "\n",
        "print(\"\\nTime Difference: Turn 2 - Walk 2 (Walking 2) :\", turn_2 - walk_2 )\n",
        "\n",
        "print(\"\\nTime Difference: Sit - Turn 2 (Turning 2) :\", sit - turn_2  )\n",
        "\n",
        "print(\"\\nTime Difference: End - Sit (Sitting) :\", end - sit  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3uBQ9ZDMDJp"
      },
      "source": [
        "--------------------\n",
        "\n",
        "Hyper tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmXZt4N6MEvA"
      },
      "source": [
        "# lstm model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.optimizers import SGD , Adam , Adamax , Adadelta , Adagrad\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ2uKkYPQkOk"
      },
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[1] , X_train.shape[2] , y_train.shape[1]\n",
        "\n",
        "# set up model\n",
        "model = Sequential()\n",
        "\n",
        "#input layer\n",
        "model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "# hidden layers\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "\n",
        "opt = Adam(learning_rate= 0.001)\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(n_outputs, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer= opt, metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPXk4LsrQozP"
      },
      "source": [
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer = Adam , neurons=1, dropout = 0 , learn_rate = 0.001 , activation = 'relu'  ):\n",
        "\n",
        "  n_timesteps, n_features, n_outputs = 10 , 9 , 6\n",
        "\n",
        "  # create model\n",
        "\n",
        "  # set up model\n",
        "  model = Sequential()\n",
        "\n",
        "  #input layer\n",
        "  model.add(LSTM(neurons, input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "  # hidden layers\n",
        "  model.add(Dense(100, activation= activation))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(50,activation= activation))\n",
        "\n",
        "\n",
        "  # output layer\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "  opt = optimizer(learning_rate=learn_rate)\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer = opt, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws_EMOzIc7Lu"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=1, batch_size=200, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi56BEgnkXfb"
      },
      "source": [
        "model.fit(X_train , y_train_arg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uuATOP6kaNv"
      },
      "source": [
        "preds = model.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYuAdzi0lKkD"
      },
      "source": [
        "print(len(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69VAv2eYk2O-"
      },
      "source": [
        "print(len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEI3AVQulJpr"
      },
      "source": [
        "max_test = np.argmax(np.asarray(y_train), axis=1)\n",
        "\n",
        "print(max_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApitEe1MRJuo"
      },
      "source": [
        "# define the grid search parameters\n",
        "\n",
        "y_train_arg =  np.argmax(np.asarray(y_train), axis = 1)\n",
        "y_test_arg =  np.argmax(np.asarray(y_test), axis = 1)\n",
        "\n",
        "neurons = [ 10, 50 , 100]\n",
        "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "learn_rate = [0.001 , 0.01]\n",
        "optimisers = [SGD, Adam, Adamax]\n",
        "batch_sizes = [50,200,1000]\n",
        "activations = ['relu' , 'tanh']\n",
        "epochs = [10, 100 , 250]\n",
        "\n",
        "\n",
        "param_grid = dict(dropout=dropout_rate, learn_rate = learn_rate, neurons =neurons , activation = activations , optimizer =optimisers , batch_size = batch_sizes , epochs = epochs) \n",
        "\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=2 ,  scoring = 'f1_weighted' , verbose = 5 )\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train_arg)\n",
        "\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThgyvZmwegUq"
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxeHXwuRehsP"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63RwfXnNejD1"
      },
      "source": [
        "gs_NN_model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMEUj8VUekxY"
      },
      "source": [
        "preds = gs_NN_model.predict(X_test)\n",
        "train_preds = gs_NN_model.predict(X_train)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train_arg, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train_arg, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train_arg, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train_arg, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test_arg, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test_arg, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test_arg, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test_arg, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}