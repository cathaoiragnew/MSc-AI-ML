{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_HC_Manual_Train_Test_Tuning_HandCraft_Thesis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EKoxDdSsr8e"
      },
      "source": [
        "# Handcrafted Features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrVWaVNkPW2k"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtX5iMQHPDII"
      },
      "source": [
        "\n",
        "########################### (https://github.com/curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs/blob/master/human_activity_recognition.ipynb) imports\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "#!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from keras import backend as K\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n",
        "########################### (https://github.com/curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs/blob/master/human_activity_recognition.ipynb) imports\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "# This is for RandomSearchCV error/delay  speeds up the searches \n",
        "from sklearn.externals.joblib import parallel_backend \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_M5F5DwPrKk"
      },
      "source": [
        "# Upload Data \n",
        "\n",
        "---------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZX9OGsWPzvD"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IyDYp-eLY8K"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_train = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXPVt1AP0v1"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDlYInx_Pyx5"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIL8q5nvqUf"
      },
      "source": [
        "### Reading in segments & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y70o5U5mSOUU"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxXU40eBttE6"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_dataframes_train = [key for key in uploaded_signal_train.keys()]\n",
        "\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_dataframe_train = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_dataframes_train)):\n",
        "\n",
        "    # load in the data \n",
        "    dataframe_train = pd.read_csv(list_of_dataframes_train[i])\n",
        "\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_dataframe_train.append(dataframe_train) \n",
        "\n",
        "\n",
        "all_df_train = pd.concat(all_dataframe_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXzYX7vVSPqK"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s9Wo8YGSQeN"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_dataframes_test = [key for key in uploaded_signal_test.keys()]\n",
        "\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_dataframe_test = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_dataframes_test)):\n",
        "\n",
        "    # load in the data \n",
        "    dataframe_test = pd.read_csv(list_of_dataframes_test[i])\n",
        "\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_dataframe_test.append(dataframe_test) \n",
        "\n",
        "\n",
        "all_df_test = pd.concat(all_dataframe_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jsSnExgGesF"
      },
      "source": [
        "# Quick Look "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCEZ0lTrGiz3"
      },
      "source": [
        "all_df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ-KTwnDGhYv"
      },
      "source": [
        "all_df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVPv0iQ6SPeH"
      },
      "source": [
        "# Train Test Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlyxtKg4SRMP"
      },
      "source": [
        "# Getting X_train & y_train'\n",
        "X_train = all_df_train.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run', 'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'\t], axis = 1)\n",
        "y_train = all_df_train['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyOL-dTlTDbw"
      },
      "source": [
        "# Getting X_train & y_train\n",
        "X_test = all_df_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run',  'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'], axis = 1)\n",
        "y_test = all_df_test['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFFnYKxVTIf6"
      },
      "source": [
        "--------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tVnwD0csNgw"
      },
      "source": [
        "Standard Scale "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFhyhbcjsO7K"
      },
      "source": [
        "# Set up ss for non-shuffled data\n",
        "ss = StandardScaler()\n",
        "\n",
        "\n",
        "# fit train & transform test\n",
        "X_train_scale = ss.fit_transform(X_train)\n",
        "X_test_scale = ss.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-zC9GFdtRZT"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUmL_yYFtS2L"
      },
      "source": [
        "# Tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9TfdnAvtXFX"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss , recall_score , f1_score, precision_score , roc_auc_score, balanced_accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmI_62XaSoYf"
      },
      "source": [
        "---------------------------\n",
        "\n",
        "MLP Classifier Tuning \n",
        "\n",
        "- alpha \n",
        "- hidden layers\n",
        "- activation\n",
        "- solver\n",
        "- learning rate "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUqrKTawtv-P"
      },
      "source": [
        "**Alpha**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpctJYCztuzJ"
      },
      "source": [
        "# Alpha values \n",
        "\n",
        "alphas = [0, 0.0001 , 0.001 , 0.01 , 0.1 ]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for a in alphas:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = MLPClassifier(alpha=a)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      alpha_min = a\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2lT6_c1wpiZ"
      },
      "source": [
        "print(f\"Max F1 score {min_error} : alpha value {alpha_min} \" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Lvz7xswqMS"
      },
      "source": [
        "plt.plot(alphas , eout ,  label = 'F1 Score'  )\n",
        "\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Test F1 Score\")\n",
        "plt.title(\"Test F1 Score Vs Alpha\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baTT1ObK2Cw0"
      },
      "source": [
        "**Hidden Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuRJttiL2H4B"
      },
      "source": [
        "# Alpha values \n",
        "\n",
        "hidden_layers = [ (50, 20) , (50,10,50), (100, 50 , 20) , (200, 50 , 100) ]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for layer in hidden_layers:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = MLPClassifier(hidden_layer_sizes= layer)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      best_layer = layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuhMq4tD7RPY"
      },
      "source": [
        "for i in range(len(eout)):\n",
        "\n",
        "  print( ein[i] , eout[i] , hidden_layers[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzSeMjT44DQ0"
      },
      "source": [
        "**Now Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVVK231t4Jsh"
      },
      "source": [
        "# Search Space\n",
        "\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50, 20) , (200, 50, 100) , (100, 50 ,20)],\n",
        "    'activation': ['tanh', 'relu', 'logistic'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': np.arange(0.00001 , 0.0005, 0.001, 0.01, 0.1),\n",
        "    'learning_rate': ['constant','adaptive']\n",
        "    \n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with parallel_backend('threading'):\n",
        "\n",
        "  gsearch_mlp = RandomizedSearchCV(estimator = MLPClassifier(),\n",
        "                        param_distributions = parameter_space,  \n",
        "                        scoring = 'f1_weighted',                \n",
        "                        cv = 3,\n",
        "                        n_jobs = 4,\n",
        "                        verbose = 5)\n",
        "  \n",
        "  grid_search_mlp = gsearch_mlp.fit(X_train_scale, y_train)\n",
        "  \n",
        "print(\"MLP Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6GdJeVguF74"
      },
      "source": [
        "gsearch_mlp.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuIavAWkuIvx"
      },
      "source": [
        "gsearch_mlp.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz1cR2truMwX"
      },
      "source": [
        "gs_mlp_model = gsearch_mlp.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBkwCUkruQml"
      },
      "source": [
        "preds = gs_mlp_model.predict(X_test_scale)\n",
        "train_preds = gs_mlp_model.predict(X_train_scale)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJhYjsQNwq_1"
      },
      "source": [
        "----------------------------------\n",
        "\n",
        "Decision Tree Tuning (https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3)\n",
        "\n",
        "- Max Depth\n",
        "- Min Sample Split\n",
        "- Min Samples Leaf \n",
        "- Criterion\n",
        "- Max Feature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScykZUT9vOO9"
      },
      "source": [
        "**Depth**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_kpa8iDvKyO"
      },
      "source": [
        "\n",
        "# Alpha values \n",
        "\n",
        "max_depth = [ 1, 5,10, 12, 15, 18, 20 , 30 ]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for depth in max_depth:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = DecisionTreeClassifier(max_depth= depth)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      best_depth = depth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP7zGixnvZdY"
      },
      "source": [
        "print(f\"Max F1 score {min_error} : depth value {best_depth} \" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_m8qNv-vbEb"
      },
      "source": [
        "for i in range(len(eout)):\n",
        "\n",
        "  print(\"Training F1-Score\", ein[i] , \"\\tTesting F1-Score\" , eout[i] , \"\\tMax Depth\", max_depth[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzERq3EJvcUc"
      },
      "source": [
        "**Min Sample Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ROr0cxDvgLM"
      },
      "source": [
        "# Alpha values \n",
        "\n",
        "sample_split = [ 2 , 10 , 25 , 50 , 100 , 150 , 200 ]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for sam_split in sample_split:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = DecisionTreeClassifier( min_samples_split= sam_split)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      best_sample_split = sam_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8VTczMovirr"
      },
      "source": [
        "print(f\"Max F1 score {min_error} : Min Sample Split best value {best_sample_split} \" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JD0APYXvjL9"
      },
      "source": [
        "for i in range(len(eout)):\n",
        "\n",
        "  print(\"Training F1-Score\", ein[i] , \"\\tTesting F1-Score\" , eout[i] , \"\\tMin Sample Split\", sample_split[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QmnsWOBvlrS"
      },
      "source": [
        "**Min Sample Leaf**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT_FjBsmvoW_"
      },
      "source": [
        "\n",
        "# Alpha values \n",
        "\n",
        "sample_leaf = [1,2,3,4,5,6,7,8,9,10,25,50]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for sam_leaf in sample_leaf:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = DecisionTreeClassifier(  min_samples_leaf=  sam_leaf)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      best_sample_leaf =  sam_leaf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8swjYy_vrHd"
      },
      "source": [
        "print(f\"Max F1 score {min_error} : min sample leaf best value {best_sample_leaf} \" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH5crTLyvsoG"
      },
      "source": [
        "for i in range(len(eout)):\n",
        "\n",
        "  print(\"Training F1-Score\", ein[i] , \"\\tTesting F1-Score\" , eout[i] , \"\\tmin sample leaf\", sample_leaf[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCqoiBbuvyDw"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEj9GpAovz1L"
      },
      "source": [
        "**Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOzuOjNzv1bS"
      },
      "source": [
        "# Hyper parameters range intialization for tuning \n",
        "\n",
        "parameters={\n",
        "\n",
        "            \"criterion\":[\"gini\",\"entropy\"],\n",
        "            \"max_depth\" : [None, 5,10, 12, 15, 18, 20 ],\n",
        "           \"min_samples_leaf\":[1 , 3,5,6,7,8,9,10],\n",
        "            \"min_samples_split\": [2 ,3, 5, 10 , 25 , 50 , 100 , 150 , 200 ],\n",
        "          #  \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
        "          #  \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90]\n",
        "            }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BE4bREJv3A1"
      },
      "source": [
        "# Tuning \n",
        "\n",
        "\n",
        "gsearch_dtc = RandomizedSearchCV(estimator = DecisionTreeClassifier(),\n",
        "                      param_distributions = parameters,  \n",
        "                      scoring = 'f1_weighted',                \n",
        "                      cv = 3,\n",
        "                      n_jobs = 4,\n",
        "                      verbose = 4)\n",
        "\n",
        "grid_search_dtc = gsearch_dtc.fit(X_train_scale, y_train)\n",
        "  \n",
        "print(\"DT Classifier Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hl8gpYPv5Yi"
      },
      "source": [
        "gsearch_dtc.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU0_i4THv54E"
      },
      "source": [
        "gsearch_dtc.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Mi1CRgv8xg"
      },
      "source": [
        "gs_dtc_model = gsearch_dtc.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SF8UDAAv9Pc"
      },
      "source": [
        "preds = gs_dtc_model.predict(X_test_scale)\n",
        "train_preds = gs_dtc_model.predict(X_train_scale)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb1C6t1cz2TX"
      },
      "source": [
        "---------------------------------------\n",
        "\n",
        "Logisitic Regression \n",
        "\n",
        "- penalty \n",
        "- C value \n",
        "- class_weight\n",
        "- solver\n",
        "- multi_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYB98RrXaIgS"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sSoWVTr0Q9r"
      },
      "source": [
        "**C Value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f_dn5nJ0TZR"
      },
      "source": [
        "\n",
        "# Alpha values \n",
        "\n",
        "C_vals = [100, 10, 1.0, 0.1, 0.01]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for c in C_vals:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = LogisticRegression( C = c)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      best_c =  c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKUk1yyr1AYD"
      },
      "source": [
        "print(f\"Max F1 score {min_error} : c value {best_c} \" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zak-e4tf1Blj"
      },
      "source": [
        "for i in range(len(eout)):\n",
        "\n",
        "  print(\"Training F1-Score\", ein[i] , \"\\tTesting F1-Score\" , eout[i] , \"\\tC values\", C_vals[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ZxU1QT1RFi"
      },
      "source": [
        "**Max Iters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdzQE2Jh1S1W"
      },
      "source": [
        "# Alpha values \n",
        "\n",
        "max_iters = [100, 200, 500]\n",
        "\n",
        "ein=[]\n",
        "eout=[]\n",
        "\n",
        "# small little tweak to return Eout min error, setting min_error extremely high so anything is better than np.inf\n",
        "min_error = 0\n",
        "i = 0 \n",
        "\n",
        "for val in max_iters:\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  # initialize model\n",
        "  clf = LogisticRegression( max_iter=  val)\n",
        "\n",
        "  # fit model\n",
        "  clf.fit(X_train_scale, y_train)\n",
        "\n",
        "  # get in sample/train f1-score \n",
        "  train_preds = clf.predict(X_train_scale)\n",
        "  train_f1 = f1_score(train_preds, y_train, average='weighted')\n",
        "  ein.append(train_f1) \n",
        "             \n",
        "  # get out of sample error \n",
        "  test_preds = clf.predict(X_test_scale)\n",
        "  test_f1 = f1_score(test_preds, y_test, average='weighted')\n",
        "  eout.append(test_f1) \n",
        "\n",
        "  i += 1\n",
        "\n",
        "  # if current error is less than minimum, update it to be minimum error and store the value of lambda for it     \n",
        "  if test_f1 > min_error:\n",
        "      min_error = test_f1\n",
        "      best_val =  val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufoGtnJY1jX4"
      },
      "source": [
        "print(f\"Max F1 score {min_error} : c value {best_val} \" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOFrwVq11jM4"
      },
      "source": [
        "for i in range(len(eout)):\n",
        "\n",
        "  print(\"Training F1-Score\", ein[i] , \"\\tTesting F1-Score\" , eout[i] , \"\\tC values\", max_iters[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrqy27p_1Cyj"
      },
      "source": [
        "# Hyper parameters range intialization for tuning \n",
        "\n",
        "parameters={\n",
        "\n",
        "            \"penalty\":[\"l1\",\"l2\"  ],\n",
        "            \"C\" : [100, 10, 1, 0.1, 0.01],\n",
        "            \"class_weight\":[None, 'balanced'],\n",
        "            \"solver\": ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "            \"multi_class\":['auto', 'ovr', 'multinomial']\n",
        "\n",
        "            }\n",
        "\n",
        "\n",
        "# Tuning \n",
        "\n",
        "gsearch_lr = RandomizedSearchCV(estimator = LogisticRegression(),\n",
        "                      param_distributions = parameters,  \n",
        "                      scoring = 'f1_weighted',                \n",
        "                      cv = 2,\n",
        "                      n_jobs = 4,\n",
        "                      verbose = 5)\n",
        "\n",
        "grid_search_lr = gsearch_lr.fit(X_train_scale, y_train)\n",
        "  \n",
        "print(\"LR Classifier Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFrRdXHj1FCO"
      },
      "source": [
        "gsearch_lr.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozCs6Hvd1Ht-"
      },
      "source": [
        "gsearch_lr.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nme04oD1JFg"
      },
      "source": [
        "gs_lr_model = gsearch_lr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHAdebqK1Ks9"
      },
      "source": [
        "preds = gs_lr_model.predict(X_test_scale)\n",
        "train_preds = gs_lr_model.predict(X_train_scale)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCs1UebVfj2b"
      },
      "source": [
        "-----------------------------\n",
        "\n",
        "Neural Nets \n",
        "\n",
        "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rer7qkKIgwoS"
      },
      "source": [
        "# lstm model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from keras.optimizers import SGD , Adam , Adamax , Adadelta , Adagrad\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04075AkPhK-5"
      },
      "source": [
        "# Getting X_train & y_train'\n",
        "X_train = all_df_train.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run', 'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'\t], axis = 1)\n",
        "#X_train = all_df_train.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run'\t], axis = 1)\n",
        "\n",
        "y_train = all_df_train['Label_segment'].values\n",
        "\n",
        "\n",
        "# convert to dummy category\n",
        "y_train_dum = pd.get_dummies(y_train)\n",
        "\n",
        "print(f'\\nDummy labels Shape {y_train_dum.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Rw8qCZhNgy"
      },
      "source": [
        "# Getting X_train & y_train\n",
        "\n",
        "X_test = all_df_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run',  'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'], axis = 1)\n",
        "#X_test = all_df_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run'\t], axis = 1)\n",
        "\n",
        "y_test = all_df_test['Label_segment'].values\n",
        "\n",
        "# convert to dummy category\n",
        "y_test_dum = pd.get_dummies(y_test)\n",
        "\n",
        "print(f'\\nDummy labels Shape {y_test_dum.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRaTtYAohRDY"
      },
      "source": [
        "# Set up ss for non-shuffled data\n",
        "ss = StandardScaler()\n",
        "\n",
        "# fit for non-shuffled\n",
        "X_train_scale = ss.fit_transform(X_train)\n",
        "X_test_scale = ss.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66B3fuCImHzl"
      },
      "source": [
        "n_outputs =  y_train_dum.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr5BvyMumIa_"
      },
      "source": [
        "n_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E352Ejdng6IH"
      },
      "source": [
        "\n",
        "n_outputs =  y_train_dum.shape[1]\n",
        "\n",
        "# set up model\n",
        "model = Sequential()\n",
        "\n",
        "#input layer\n",
        "model.add(Dense(100, activation='relu' , input_shape = (X_train_scale.shape[1], )))\n",
        "\n",
        "# hidden layers\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwW_7ptmfpSv"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer = Adam , neurons=1, dropout = 0 , learn_rate = 0.001 , activation = 'relu' , ):\n",
        "\n",
        "  # create model\n",
        "\n",
        "  # set up model\n",
        "  model = Sequential()\n",
        "\n",
        "  #input layer\n",
        "  model.add(Dense(neurons, activation= activation , input_shape = (X_train_scale.shape[1], )))\n",
        "\n",
        "  # hidden layers\n",
        "  model.add(Dense(neurons, activation= activation))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(neurons, activation= activation))\n",
        "\n",
        "\n",
        "  # output layer\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "  opt = optimizer(learning_rate=learn_rate)\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer = opt, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8wQ52D3iELI"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=200, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWjVrIfJiMNi"
      },
      "source": [
        "# define the grid search parameters\n",
        "\n",
        "neurons = [ 10, 50 , 100]\n",
        "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "learn_rate = [0.001 , 0.01]\n",
        "optimisers = [SGD, Adam, Adamax]\n",
        "batch_sizes = [50,200,1000]\n",
        "activations = ['relu' , 'tanh']\n",
        "epochs = [10, 100 , 250]\n",
        "\n",
        "\n",
        "param_grid = dict(dropout=dropout_rate, neurons =neurons , learn_rate = learn_rate , activation = activations , optimizer =optimisers , batch_size = batch_sizes , epochs = epochs) \n",
        "\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=2 ,  scoring = 'f1_weighted' , verbose = 5 )\n",
        "\n",
        "grid_result = grid.fit(X_train_scale, y_train)\n",
        "\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvj-8AgZx0bt"
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24hkzgCax0by"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYhEyRr8x0bz"
      },
      "source": [
        "gs_NN_model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAZNW8Apx0b0"
      },
      "source": [
        "preds = gs_NN_model.predict(X_test_scale)\n",
        "train_preds = gs_NN_model.predict(X_train_scale)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjirTIZH8Avi"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# set up labels \n",
        "LABELS = ['Go', 'Turn1',  'Turn2' , 'Walk1', 'Walk2', 'Sit']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QRxQP4H8DxU"
      },
      "source": [
        "# Test \n",
        "print(classification_report(y_test, preds , target_names=LABELS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1CNzIxL_ZGT"
      },
      "source": [
        "n_ouputs = y_train_dum.shape[1]\n",
        "\n",
        "# set up model\n",
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Dense(100, activation='relu' , input_shape = (X_train_scale.shape[1], )))\n",
        "\n",
        "# hidden layers\n",
        "model.add(Dense(100, activation='relu' ))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(100, activation='relu' ))\n",
        "\n",
        "# output layer \n",
        "model.add(Dense(n_outputs , activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.001)\n",
        "\n",
        "# compile model \n",
        "model.compile(loss= 'categorical_crossentropy' , optimizer= opt , metrics= [tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24Zw3F9M8SeK"
      },
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, preds)\n",
        "confusion_matrix_t = metrics.confusion_matrix(y_train, train_preds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO9uGlEb8ZZ7"
      },
      "source": [
        "# Test \n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True , fmt=\"d\");\n",
        "plt.title(\"Test Data Confusion matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzD1qmQM8Dip"
      },
      "source": [
        "# Train\n",
        "print(classification_report(y_train, train_preds , target_names=LABELS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U2yXbql8R_Y"
      },
      "source": [
        "# Train\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(confusion_matrix_t, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Train Confusion matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}