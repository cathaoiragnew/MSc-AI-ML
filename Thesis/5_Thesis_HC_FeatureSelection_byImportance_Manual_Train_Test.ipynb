{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_FeatureSelection_byImportance_Manual_Train_Test_HandCraft_Thesis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrVWaVNkPW2k"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtX5iMQHPDII"
      },
      "source": [
        "\n",
        "########################### (https://github.com/curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs/blob/master/human_activity_recognition.ipynb) imports\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "#!pip install imbalanced-learn\n",
        "\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_M5F5DwPrKk"
      },
      "source": [
        "# Upload Data \n",
        "\n",
        "---------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZX9OGsWPzvD"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IyDYp-eLY8K"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_train = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXPVt1AP0v1"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDlYInx_Pyx5"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIL8q5nvqUf"
      },
      "source": [
        "### Reading in segments & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y70o5U5mSOUU"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxXU40eBttE6"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_dataframes_train = [key for key in uploaded_signal_train.keys()]\n",
        "\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_dataframe_train = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_dataframes_train)):\n",
        "\n",
        "    # load in the data \n",
        "    dataframe_train = pd.read_csv(list_of_dataframes_train[i])\n",
        "\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_dataframe_train.append(dataframe_train) \n",
        "\n",
        "\n",
        "all_df_train = pd.concat(all_dataframe_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXzYX7vVSPqK"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s9Wo8YGSQeN"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_dataframes_test = [key for key in uploaded_signal_test.keys()]\n",
        "\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_dataframe_test = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_dataframes_test)):\n",
        "\n",
        "    # load in the data \n",
        "    dataframe_test = pd.read_csv(list_of_dataframes_test[i])\n",
        "\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_dataframe_test.append(dataframe_test) \n",
        "\n",
        "\n",
        "all_df_test = pd.concat(all_dataframe_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jsSnExgGesF"
      },
      "source": [
        "# Quick Look "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCEZ0lTrGiz3"
      },
      "source": [
        "all_df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ-KTwnDGhYv"
      },
      "source": [
        "all_df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVPv0iQ6SPeH"
      },
      "source": [
        "# Train Test Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlyxtKg4SRMP"
      },
      "source": [
        "# Getting X_train & y_train'\n",
        "X_train = all_df_train.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run', 'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'\t], axis = 1)\n",
        "y_train = all_df_train['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyOL-dTlTDbw"
      },
      "source": [
        "# Getting X_train & y_train\n",
        "X_test = all_df_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run',  'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'], axis = 1)\n",
        "y_test = all_df_test['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vM6doFdbvla"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjWe75kFJjsl"
      },
      "source": [
        "Add Shuffle just incase it might help "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHfw8OARJmfr"
      },
      "source": [
        "all_df_train_shuffled = shuffle(all_df_train)\n",
        "all_df_test_shuffled = shuffle(all_df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgNQHv2cJ4A6"
      },
      "source": [
        "# Getting X_train & y_train\n",
        "X_train_sh = all_df_train.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run', 'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'], axis = 1)\n",
        "y_train_sh = all_df_train['Label_segment'].values\n",
        "\n",
        "# Getting X_train & y_train\n",
        "X_test_sh = all_df_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run', 'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'], axis = 1)\n",
        "y_test_sh = all_df_test['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFFnYKxVTIf6"
      },
      "source": [
        "--------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tVnwD0csNgw"
      },
      "source": [
        "Standard Scale "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFhyhbcjsO7K"
      },
      "source": [
        "# Set up ss for non-shuffled data\n",
        "ss = StandardScaler()\n",
        "# Set up ss for shuffled data\n",
        "ss_sh = StandardScaler()\n",
        "\n",
        "# fit for non-shuffled\n",
        "X_train_scale = ss.fit_transform(X_train)\n",
        "X_test_scale = ss.transform(X_test)\n",
        "\n",
        "# fit for shuffled\n",
        "X_train_sh_scale = ss_sh.fit_transform(X_train_sh)\n",
        "X_test_sh_scale = ss_sh.transform(X_test_sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZSss6u0jQN_"
      },
      "source": [
        "----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJaGvDf1jkFt"
      },
      "source": [
        "### Non Shuffled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJbtjHXbjRUr"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss , recall_score , f1_score, precision_score , roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier , LogisticRegression\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "classifiers = [\n",
        "               \n",
        "    DecisionTreeClassifier(criterion = 'entropy' , max_depth = 10 , min_samples_leaf = 8 , min_samples_split = 3  ),\n",
        "    MLPClassifier(hidden_layer_sizes = (200, 50 , 100) , activation = 'tanh' , solver = 'sgd' , alpha = 0.00001 , learning_rate = 'adaptive' ),\n",
        "    LogisticRegression(C = 0.01 , class_weight= 'balanced' , multi_class = 'auto' , penalty = 'l2' , solver = 'liblinear' )\n",
        "\n",
        "\n",
        "      ]\n",
        "\n",
        "\n",
        "# list to hold for dataframe\n",
        "classifier_name_list = []\n",
        "train_acc_list = []\n",
        "train_bacc_list = []\n",
        "test_acc_list = []\n",
        "test_bacc_list = []\n",
        "train_recall_list = []\n",
        "test_recall_list = []\n",
        "train_precision_list = []\n",
        "test_precision_list = []\n",
        "train_f1_list = []\n",
        "test_f1_list = []\n",
        "\n",
        "\n",
        "for clf in classifiers:\n",
        "    clf.fit(X_train_scale, y_train)\n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    print(name)\n",
        "    \n",
        "    print('****Results****')\n",
        "\n",
        "    # Test Predictions\n",
        "    test_predictions = clf.predict(X_test_scale)\n",
        "\n",
        "    # Test Metrics\n",
        "    acc            = accuracy_score(y_test, test_predictions)\n",
        "    bal_acc        = balanced_accuracy_score(y_test, test_predictions)\n",
        "    recall_test    = recall_score(y_test, test_predictions, average = 'weighted')\n",
        "    f1_test        = f1_score(y_test, test_predictions ,  average = 'weighted')\n",
        "    precision_test = precision_score(y_test, test_predictions,  average = 'weighted') \n",
        "\n",
        "\n",
        "\n",
        "    # Train Predictions\n",
        "    train_predictions = clf.predict(X_train_scale)\n",
        "\n",
        "    # Train Metrics\n",
        "    train_acc       = accuracy_score(y_train, train_predictions)\n",
        "    train_bal_acc   = balanced_accuracy_score(y_train, train_predictions)\n",
        "    recall_train    = recall_score(y_train, train_predictions , average = 'weighted')\n",
        "    f1_train        = f1_score(y_train, train_predictions ,  average = 'weighted')\n",
        "    precision_train = precision_score(y_train, train_predictions,  average = 'weighted') \n",
        "\n",
        "    print(\"\\n\\nTest Classification Report\\n\")\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # append to list to make a dataframe \n",
        "    classifier_name_list.append(name)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(acc)\n",
        "\n",
        "    train_bacc_list.append(train_bal_acc)\n",
        "    test_bacc_list.append(bal_acc)\n",
        "\n",
        "    train_recall_list.append(recall_train)\n",
        "    test_recall_list.append(recall_test)\n",
        "\n",
        "    train_precision_list.append(precision_train)\n",
        "    test_precision_list.append(precision_test)\n",
        "\n",
        "    train_f1_list.append(f1_train)\n",
        "    test_f1_list.append(f1_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kIM7401jneh"
      },
      "source": [
        "metrics_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "metrics_df['Classifier'] = classifier_name_list\n",
        "\n",
        "# F1 Score First \n",
        "metrics_df['Train F1'] = train_f1_list\n",
        "metrics_df['Test F1'] = test_f1_list \n",
        "\n",
        "# Recall\n",
        "metrics_df['Train Recall'] = train_recall_list\n",
        "metrics_df['Test Recall'] = test_recall_list\n",
        "\n",
        "# Precision \n",
        "metrics_df['Train Precision'] = train_precision_list\n",
        "metrics_df['Test Precision'] = test_precision_list\n",
        "\n",
        "# Bal Acc\n",
        "metrics_df['Train Balanced Accuracy'] = train_bacc_list\n",
        "metrics_df['Test Balanced Accuracy'] = test_bacc_list\n",
        "\n",
        "# Accuracy \n",
        "metrics_df['Train Accuracy'] = train_acc_list\n",
        "metrics_df['Test Accuracy'] = test_acc_list \n",
        "\n",
        "\n",
        "metrics_df.sort_values(\"Test F1\" , ascending=False , inplace=True)\n",
        "metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "# display df\n",
        "display(metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL24tHHkjiKV"
      },
      "source": [
        "### Shuffled "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJftrbqDjmJd"
      },
      "source": [
        "# same code \n",
        "\n",
        "classifiers = [\n",
        "    # KNeighborsClassifier(3),\n",
        "    # DecisionTreeClassifier(),\n",
        "    # RandomForestClassifier(),\n",
        "    # AdaBoostClassifier(),\n",
        "    # GradientBoostingClassifier(),\n",
        "    # GaussianNB(),\n",
        "    # LinearDiscriminantAnalysis(),\n",
        "    # QuadraticDiscriminantAnalysis(),\n",
        "    # LinearSVC(),\n",
        "    # SGDClassifier(),\n",
        "    # MLPClassifier(),\n",
        "    # PassiveAggressiveClassifier(),\n",
        "    # LabelPropagation(),\n",
        "    # ExtraTreesClassifier(),\n",
        "    # BaggingClassifier(),\n",
        "    # DecisionTreeClassifier(),\n",
        "    # ExtraTreeClassifier()\n",
        "\n",
        "    DecisionTreeClassifier(criterion = 'entropy' , max_depth = 10 , min_samples_leaf = 8 , min_samples_split = 3  ),\n",
        "    MLPClassifier(hidden_layer_sizes = (200, 50 , 100) , activation = 'tanh' , solver = 'sgd' , alpha = 0.00001 , learning_rate = 'adaptive' ),\n",
        "    LogisticRegression(C = 0.01 , class_weight= 'balanced' , multi_class = 'auto' , penalty = 'l2' , solver = 'liblinear' )\n",
        "\n",
        "  \n",
        "    \n",
        "      ]\n",
        "\n",
        "\n",
        "# list to hold for dataframe\n",
        "classifier_name_list = []\n",
        "train_acc_list = []\n",
        "train_bacc_list = []\n",
        "test_acc_list = []\n",
        "test_bacc_list = []\n",
        "train_recall_list = []\n",
        "test_recall_list = []\n",
        "train_precision_list = []\n",
        "test_precision_list = []\n",
        "train_f1_list = []\n",
        "test_f1_list = []\n",
        "\n",
        "\n",
        "for clf in classifiers:\n",
        "    clf.fit(X_train_sh_scale, y_train_sh)\n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    print(name)\n",
        "    \n",
        "    print('****Results****')\n",
        "\n",
        "    # Test Predictions\n",
        "    test_predictions = clf.predict(X_test_sh_scale)\n",
        "\n",
        "    # Test Metrics\n",
        "    acc            = accuracy_score(y_test_sh, test_predictions)\n",
        "    bal_acc        = balanced_accuracy_score(y_test_sh, test_predictions)\n",
        "    recall_test    = recall_score(y_test_sh, test_predictions, average = 'weighted')\n",
        "    f1_test        = f1_score(y_test_sh, test_predictions ,  average = 'weighted')\n",
        "    precision_test = precision_score(y_test_sh, test_predictions,  average = 'weighted') \n",
        "\n",
        "\n",
        "\n",
        "    # Train Predictions\n",
        "    train_predictions = clf.predict(X_train_sh_scale)\n",
        "\n",
        "    # Train Metrics\n",
        "    train_acc       = accuracy_score(y_train_sh, train_predictions)\n",
        "    train_bal_acc   = balanced_accuracy_score(y_train_sh, train_predictions)\n",
        "    recall_train    = recall_score(y_train_sh, train_predictions , average = 'weighted')\n",
        "    f1_train        = f1_score(y_train_sh, train_predictions ,  average = 'weighted')\n",
        "    precision_train = precision_score(y_train_sh, train_predictions,  average = 'weighted') \n",
        "\n",
        "    print(\"\\n\\nTest Classification Report\\n\")\n",
        "    print(classification_report(y_test_sh, test_predictions))\n",
        "\n",
        "    # append to list to make a dataframe \n",
        "    classifier_name_list.append(name)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(acc)\n",
        "\n",
        "    train_bacc_list.append(train_bal_acc)\n",
        "    test_bacc_list.append(bal_acc)\n",
        "\n",
        "    train_recall_list.append(recall_train)\n",
        "    test_recall_list.append(recall_test)\n",
        "\n",
        "    train_precision_list.append(precision_train)\n",
        "    test_precision_list.append(precision_test)\n",
        "\n",
        "    train_f1_list.append(f1_train)\n",
        "    test_f1_list.append(f1_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Ha-uiFjzC0"
      },
      "source": [
        "metrics_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "metrics_df['Classifier'] = classifier_name_list\n",
        "\n",
        "# F1 Score First \n",
        "metrics_df['Train F1'] = train_f1_list\n",
        "metrics_df['Test F1'] = test_f1_list \n",
        "\n",
        "# Recall\n",
        "metrics_df['Train Recall'] = train_recall_list\n",
        "metrics_df['Test Recall'] = test_recall_list\n",
        "\n",
        "# Precision \n",
        "metrics_df['Train Precision'] = train_precision_list\n",
        "metrics_df['Test Precision'] = test_precision_list\n",
        "\n",
        "# Bal Acc\n",
        "metrics_df['Train Balanced Accuracy'] = train_bacc_list\n",
        "metrics_df['Test Balanced Accuracy'] = test_bacc_list\n",
        "\n",
        "# Accuracy \n",
        "metrics_df['Train Accuracy'] = train_acc_list\n",
        "metrics_df['Test Accuracy'] = test_acc_list \n",
        "\n",
        "\n",
        "metrics_df.sort_values(\"Test F1\" , ascending=False , inplace=True)\n",
        "metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "# display df\n",
        "display(metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-l6JVkzlASd"
      },
      "source": [
        "Seemginly no difference between shuffled vs no shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ppjtabjRi-"
      },
      "source": [
        "---------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7tua6JR_bFG"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t93SCoYnmQwz"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omk8aHiRsMQg"
      },
      "source": [
        "# Check Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7vIiO37JcMX"
      },
      "source": [
        "clf_dtc = DecisionTreeClassifier()\n",
        "\n",
        "clf_dtc.fit(X_train_scale , y_train)\n",
        "\n",
        "preds_dtc = clf_dtc.predict(X_test_scale)\n",
        "\n",
        "\n",
        "print(\"Test Accuracy : \\t\\t\" ,accuracy_score(y_test, preds_dtc))\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds_dtc))\n",
        "#print(\"F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3k-7P2NKbQ3"
      },
      "source": [
        "\n",
        "# Shuffled Data \n",
        "\n",
        "clf_dtc_sh = DecisionTreeClassifier()\n",
        "\n",
        "clf_dtc_sh.fit(X_train_sh_scale , y_train_sh)\n",
        "\n",
        "preds_dtc_sh = clf_dtc_sh.predict(X_test_sh_scale)\n",
        "\n",
        "\n",
        "print(\"Test Shuffled Accuracy : \\t\\t\" ,accuracy_score(y_test_sh, preds_rfc_sh))\n",
        "print(\"Test Shuffled Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test_sh, preds_rfc_sh))\n",
        "#print(\"F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-orrv2jtS8bO"
      },
      "source": [
        "----------------------------------------------------\n",
        "\n",
        "# Feature Importance from RFC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyMkwy7oUDwb"
      },
      "source": [
        "\n",
        "figure(figsize=(20, 20), dpi=80)\n",
        "\n",
        "plt.title(\"Feature Importance\")\n",
        "(pd.Series(clf_dtc.feature_importances_, index=X_train.columns).plot(kind='barh'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1seHkdoRWDgY"
      },
      "source": [
        "\n",
        "figure(figsize=(15, 15), dpi=80)\n",
        "\n",
        "plt.title(\"Feature Importance\")\n",
        "(pd.Series(clf_dtc.feature_importances_, index=X_train.columns).nlargest(30).plot(kind='barh'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMgfpsWRS_ZD"
      },
      "source": [
        "# get importance\n",
        "importance = clf_dtc.feature_importances_\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7z-LXtYm-Zq"
      },
      "source": [
        "# Feature Selection "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EfHqDrqZNnB"
      },
      "source": [
        "# configure to select a subset of features\n",
        "fs = SelectFromModel(DecisionTreeClassifier(n_estimators=100), max_features=30)\n",
        "\n",
        "\n",
        "# learn relationship from training data\n",
        "fs.fit(X_train_scale , y_train)\n",
        "\n",
        "# transform train input data\n",
        "X_train_scale_fs = fs.transform(X_train_scale)\n",
        "# transform test input data\n",
        "X_test_scale_fs = fs.transform(X_test_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62SrFX2WnA1V"
      },
      "source": [
        "# Classifier after selection on features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQu-Knv1Z5ON"
      },
      "source": [
        "clf_dtc_fs = DecisionTreeClassifier()\n",
        "\n",
        "clf_dtc_fs.fit(X_train_scale_fs , y_train)\n",
        "\n",
        "preds_dtc_fs= clf_dtc_fs.predict(X_train_scale_fs )\n",
        "\n",
        "\n",
        "print(\"Train Accuracy : \\t\\t\" ,accuracy_score(y_train, preds_dtc_fs))\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, preds_dtc_fs))\n",
        "#print(\"F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGYM9nuGatLe"
      },
      "source": [
        "# test\n",
        "preds_dtc_fs_test = clf_dtc_fs.predict(X_test_scale_fs )\n",
        "\n",
        "\n",
        "print(\"Accuracy : \\t\\t\" ,accuracy_score(y_test, preds_dtc_fs_test))\n",
        "print(\"Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds_dtc_fs_test))\n",
        "#print(\"F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja8yc5zanet7"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "531_LizDnM39"
      },
      "source": [
        "Can drop over 50 features and maintain approx the same percentage range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwEu2GF8JcvA"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOfCeNXSVpxp"
      },
      "source": [
        "Now check on all baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IGWqg0fVokn"
      },
      "source": [
        "\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, log_loss , recall_score , f1_score, precision_score , roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier , LogisticRegression\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifiers = [\n",
        "               \n",
        "    # Too much computation to run now since using entire data and not segmenting on location \n",
        "    ##KNeighborsClassifier(),\n",
        "    ##GradientBoostingClassifier(),\n",
        "    ##LabelPropagation(),\n",
        "    # DecisionTreeClassifier(),\n",
        "    # RandomForestClassifier(),\n",
        "    # AdaBoostClassifier(),\n",
        "    # GaussianNB(),\n",
        "    # LinearDiscriminantAnalysis(),\n",
        "    # QuadraticDiscriminantAnalysis(),\n",
        "    # LinearSVC(),\n",
        "    # SGDClassifier(),\n",
        "    # MLPClassifier(),\n",
        "    # PassiveAggressiveClassifier(),\n",
        "    # ExtraTreesClassifier(),\n",
        "    # BaggingClassifier(),\n",
        "\n",
        "    DecisionTreeClassifier(criterion = 'entropy' , max_depth = 10 , min_samples_leaf = 8 , min_samples_split = 3  ),\n",
        "    MLPClassifier(hidden_layer_sizes = (200, 50 , 100) , activation = 'tanh' , solver = 'sgd' , alpha = 0.00001 , learning_rate = 'adaptive' ),\n",
        "    LogisticRegression(C = 1 , multi_class = 'multinomial' , penalty = 'l2' , solver = 'newton-cg' )\n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "     ]\n",
        "\n",
        "\n",
        "# list to hold for dataframe\n",
        "classifier_name_list = []\n",
        "train_acc_list = []\n",
        "train_bacc_list = []\n",
        "test_acc_list = []\n",
        "test_bacc_list = []\n",
        "train_recall_list = []\n",
        "test_recall_list = []\n",
        "train_precision_list = []\n",
        "test_precision_list = []\n",
        "train_f1_list = []\n",
        "test_f1_list = []\n",
        "training_timing = [] \n",
        "training_pred_timing = [] \n",
        "testing_pred_timing = [] \n",
        "\n",
        "\n",
        "for clf in classifiers:\n",
        "\n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    print(name)\n",
        "\n",
        "    # just for timing model\n",
        "    training_time0 = time.time()\n",
        "\n",
        "\n",
        "    ################### This changes across runs ####################################################\n",
        "    clf.fit(X_train_scale_fs , y_train)\n",
        "\n",
        "    # finished training \n",
        "    training_time1 = time.time()\n",
        "\n",
        "    training_time = training_time1 - training_time0 \n",
        "    \n",
        "    print('****Results****')\n",
        "\n",
        "    # just for timing model\n",
        "    test_time0 = time.time()\n",
        "    # Test Predictions\n",
        "\n",
        "    ################### This changes across runs ####################################################\n",
        "    test_predictions = clf.predict(X_test_scale_fs)\n",
        "    # just for timing model\n",
        "    test_time1 = time.time()\n",
        "\n",
        "    test_time = test_time1 - test_time0\n",
        "\n",
        "\n",
        "    # Test Metrics\n",
        "    acc            = accuracy_score(y_test, test_predictions)\n",
        "    bal_acc        = balanced_accuracy_score(y_test, test_predictions)\n",
        "    recall_test    = recall_score(y_test, test_predictions, average = 'weighted')\n",
        "    f1_test        = f1_score(y_test, test_predictions ,  average = 'weighted')\n",
        "    precision_test = precision_score(y_test, test_predictions,  average = 'weighted') \n",
        "\n",
        "\n",
        "    # just for timing model\n",
        "    training_p_time0 = time.time()\n",
        "\n",
        "    # Train Predictions\n",
        "\n",
        "    ################### This changes across runs ####################################################\n",
        "    train_predictions = clf.predict(X_train_scale_fs)\n",
        "\n",
        "    # just for timing model\n",
        "    training_p_time1 = time.time()\n",
        "\n",
        "    training_p_time = training_p_time1 - training_p_time0\n",
        "\n",
        "    # Train Metrics\n",
        "\n",
        "    ################### This changes across runs ####################################################\n",
        "    train_acc       = accuracy_score(y_train, train_predictions)\n",
        "    train_bal_acc   = balanced_accuracy_score(y_train, train_predictions)\n",
        "    recall_train    = recall_score(y_train, train_predictions , average = 'weighted')\n",
        "    f1_train        = f1_score(y_train, train_predictions ,  average = 'weighted')\n",
        "    precision_train = precision_score(y_train, train_predictions,  average = 'weighted') \n",
        "\n",
        "    print(\"\\n\\nTest Classification Report\\n\")\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "\n",
        "    # append to list to make a dataframe \n",
        "    classifier_name_list.append(name)\n",
        "    \n",
        "    training_timing.append(training_time)\n",
        "    training_pred_timing.append(training_p_time)\n",
        "    testing_pred_timing.append(test_time)\n",
        "\n",
        "\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(acc)\n",
        "\n",
        "    train_bacc_list.append(train_bal_acc)\n",
        "    test_bacc_list.append(bal_acc)\n",
        "\n",
        "    train_recall_list.append(recall_train)\n",
        "    test_recall_list.append(recall_test)\n",
        "\n",
        "    train_precision_list.append(precision_train)\n",
        "    test_precision_list.append(precision_test)\n",
        "\n",
        "    train_f1_list.append(f1_train)\n",
        "    test_f1_list.append(f1_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcjTq2sBXuEG"
      },
      "source": [
        "train_metrics_df = pd.DataFrame()\n",
        "test_metrics_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "train_metrics_df['Classifier'] = classifier_name_list\n",
        "test_metrics_df['Classifier'] = classifier_name_list\n",
        "\n",
        "\n",
        "# F1 Score First \n",
        "train_metrics_df['Train F1'] = train_f1_list\n",
        "test_metrics_df['Test F1'] = test_f1_list \n",
        "\n",
        "# Recall\n",
        "train_metrics_df['Train Recall'] = train_recall_list\n",
        "test_metrics_df['Test Recall'] = test_recall_list\n",
        "\n",
        "# Precision \n",
        "train_metrics_df['Train Precision'] = train_precision_list\n",
        "test_metrics_df['Test Precision'] = test_precision_list\n",
        "\n",
        "# Bal Acc\n",
        "train_metrics_df['Train Balanced Accuracy'] = train_bacc_list\n",
        "test_metrics_df['Test Balanced Accuracy'] = test_bacc_list\n",
        "\n",
        "# Accuracy \n",
        "train_metrics_df['Train Accuracy'] = train_acc_list\n",
        "test_metrics_df['Test Accuracy'] = test_acc_list \n",
        "\n",
        "\n",
        "#train_metrics_df.sort_values(\"Train F1\" , ascending=False , inplace=True)\n",
        "train_metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "#test_metrics_df.sort_values(\"Test F1\" , ascending=False , inplace=True)\n",
        "test_metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "train_metrics_df['Model Training Taken (seconds)']  = training_timing\n",
        "test_metrics_df['Model Training Taken (seconds)']  = training_timing\n",
        "\n",
        "train_metrics_df['Model Prediction Time Taken (seconds)']  = training_pred_timing\n",
        "test_metrics_df['Model Prediction Time Taken (seconds)']  = testing_pred_timing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display df\n",
        "display(train_metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSVcq7wPXv0j"
      },
      "source": [
        "# display df\n",
        "display(test_metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGvZS8HVnVOJ"
      },
      "source": [
        "----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wfSaGW-pmlT"
      },
      "source": [
        "# XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRmUSKjfrHbZ"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIY6_QRApply"
      },
      "source": [
        "# Random Grid Search to Optimize model\n",
        "\n",
        "# set up parameter grid\n",
        "parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n",
        "              \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1],\n",
        "              \"max_depth\": [2, 5, 10],\n",
        "              \"reg_alpha\": [0, 0.5, 1],\n",
        "              \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
        "              \"min_child_weight\": [1, 3, 5],\n",
        "              \"n_estimators\": [10, 50, 100]\n",
        "\n",
        "              }\n",
        "\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "gsearch = RandomizedSearchCV(estimator = xgb_clf,\n",
        "                        param_distributions = parameters,  \n",
        "                        scoring = 'balanced_accuracy',                \n",
        "                        cv = 2,\n",
        "                        n_jobs = -1,\n",
        "                        verbose = 4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyTQa3-0sUY6"
      },
      "source": [
        "grid_search = gsearch.fit(X_train_scale_fs, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE-Dsbx1vyAb"
      },
      "source": [
        "gs_model = grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNyE3oKiv7iH"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjCL_3FUb9q4"
      },
      "source": [
        "from sklearn.metrics import balanced_accuracy_score , recall_score , precision_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PITM64R_wOLO"
      },
      "source": [
        "xgb_clf.fit(X_train_scale, y_train)\n",
        "\n",
        "preds = xgb_clf.predict(X_test_scale)\n",
        "\n",
        "\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8c_TrJYwxKH"
      },
      "source": [
        "gs_model.fit(X_train_scale_fs, y_train)\n",
        "\n",
        "preds_gs = gs_model.predict(X_test_scale_fs)\n",
        "\n",
        "print(balanced_accuracy_score(y_test, preds_gs ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eunoyu7xQi8"
      },
      "source": [
        "-------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wziyrc4QxR3v"
      },
      "source": [
        "def holdout_checker_xgb(smote = False ):\n",
        "\n",
        "  # Read in Fresh Data to see what predictions looks like\n",
        "  from google.colab import files\n",
        "  uploaded_test = files.upload()\n",
        "\n",
        "  # getting keys, which is file names of csv\n",
        "  val_file_name = [key for key in uploaded_test.keys()]\n",
        "\n",
        "  # read in csv \n",
        "  signal_test = pd.read_csv(val_file_name[0])\n",
        "\n",
        "\n",
        "  # Getting X_train & y_train\n",
        "  X_data = signal_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run','X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands' ], axis = 1)\n",
        "  y_data = signal_test['Label_segment'].values\n",
        "\n",
        "  # scale with already fit ss \n",
        "  X_data_scale = ss.transform(X_data)\n",
        "\n",
        "  # feature selection\n",
        "  #X_data_scale_fs = fs.transform(X_data_scale)\n",
        "\n",
        "\n",
        "  # if selecting smote trained model\n",
        "  if smote == True:\n",
        "    hold_preds = model_smote.predict(X_data_scale_fs)\n",
        "\n",
        "  # else non-smote trained model\n",
        "  else:\n",
        "    hold_preds = xgb_clf.predict(X_data_scale)\n",
        "\n",
        "  # take max of predictions \n",
        "  #max_predictions = np.argmax(hold_preds, axis=1)\n",
        "\n",
        "  # metrics \n",
        "  print(\"\\n---------------------- Metrics ----------------------------------------\")\n",
        "\n",
        "  print(\"Accuracy : \\t\\t\" ,accuracy_score(y_data, hold_preds))\n",
        "  print(\"Balanced Accuracy : \\t\" , balanced_accuracy_score(y_data, hold_preds))\n",
        "  #print(\"F1 Score : \\t\\t\" , f1_score(y_data, hold_preds, average='weighted'))\n",
        "\n",
        "\n",
        "  # set up labels \n",
        "  LABELS = ['Go', 'Turn1',  'Turn2' , 'Walk1', 'Walk2', 'Sit']\n",
        "\n",
        "  # classification report \n",
        "  print(\"\\n------------------- HoldOut Classification Report ---------------\")\n",
        "  print(classification_report(y_data , hold_preds))\n",
        "  print(\" \")\n",
        "\n",
        "  # confusion matrix\n",
        "  confusion_matrix_out = metrics.confusion_matrix(y_data, hold_preds )\n",
        "\n",
        "  plt.figure(figsize=(14, 10))\n",
        "  sns.heatmap(confusion_matrix_out, xticklabels=LABELS, yticklabels=LABELS, annot=True ,fmt=\"d\" );\n",
        "  plt.title(\"HoldOut Data Confusion matrix\")\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "  # lastly just printing actual predictions\n",
        "  print(\"\\n0 = Go \\t\\t 1 = Turn 1 \\t 2 = Turn 2\")\n",
        "  print(\"\\n3 = Walk 1 \\t 4 = Walk 2 \\t 5 = Sit\")\n",
        "\n",
        "  print(\" \")\n",
        "  print(hold_preds)\n",
        "\n",
        "  print(Counter(hold_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk73T1exzIB6"
      },
      "source": [
        "holdout_checker_xgb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsciFRW03oBc"
      },
      "source": [
        "----------------------------------------\n",
        "\n",
        "K NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKA_sIx83pHo"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import f1_score , precision_score , recall_score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TUG-MksY394p"
      },
      "source": [
        "acc_list       = []\n",
        "bal_acc_list   = []\n",
        "recall_list    = [] \n",
        "precision_list = []\n",
        "f1_list        = []\n",
        "\n",
        "\n",
        "for i in range(1,40, 2):\n",
        "\n",
        "  # track run\n",
        "  print(i)\n",
        " \n",
        "  # fitting & predicitons \n",
        "  knn = KNeighborsClassifier(n_neighbors=i)\n",
        "  knn.fit(X_train_scale, y_train)\n",
        "  pred_i = knn.predict(X_test_scale)\n",
        "\n",
        "  # Metrics of interest results\n",
        "  acc_i       = accuracy_score(y_test ,pred_i)\n",
        "  bal_acc_i   = balanced_accuracy_score(y_test ,pred_i)\n",
        "  recall_i    = recall_score(y_test , pred_i , average = 'weighted')\n",
        "  f1_i        = f1_score(y_test , pred_i ,  average = 'weighted')\n",
        "  precision_i = precision_score(y_test , pred_i,  average = 'weighted') \n",
        "\n",
        "  # append results \n",
        "  acc_list.append(acc_i)\n",
        "  bal_acc_list.append(bal_acc_i)\n",
        "  recall_list.append(recall_i)\n",
        "  precision_list.append(f1_i)\n",
        "  f1_list.append(precision_i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-FKv2F4kO7"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.plot(range(1,40,2),bal_acc_list,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
        "plt.title('Balanced Accuracy vs. K Value')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Balanced Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5QigNK7LqPL"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.plot(range(1,40,2),f1_list,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
        "plt.title('F1 Score vs. K Value')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('F1 Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5os-gTk63Js"
      },
      "source": [
        "# K = 17\n",
        "\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=7)\n",
        "knn_clf.fit(X_train_scale_fs, y_train)\n",
        "preds_knn = knn_clf.predict(X_test_scale_fs)\n",
        "\n",
        "\n",
        "print(\"Accuracy : \\t\\t\" ,accuracy_score(y_test, preds_knn))\n",
        "print(\"Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds_knn))\n",
        "#print(\"F1 Score : \\t\\t\" , f1_score(y_data, hold_preds, average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaLAaL3_7apK"
      },
      "source": [
        "def holdout_checker_knn(smote = False ):\n",
        "\n",
        "  # Read in Fresh Data to see what predictions looks like\n",
        "  from google.colab import files\n",
        "  uploaded_test = files.upload()\n",
        "\n",
        "  # getting keys, which is file names of csv\n",
        "  val_file_name = [key for key in uploaded_test.keys()]\n",
        "\n",
        "  # read in csv \n",
        "  signal_test = pd.read_csv(val_file_name[0])\n",
        "\n",
        "\n",
        "  # Getting X_train & y_train\n",
        "  X_data = signal_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run','X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands' ], axis = 1)\n",
        "  y_data = signal_test['Label_segment'].values\n",
        "\n",
        "  # scale with already fit ss \n",
        "  X_data_scale = ss.transform(X_data)\n",
        "\n",
        "  # feature selection\n",
        "  X_data_scale_fs = fs.transform(X_data_scale)\n",
        "\n",
        "\n",
        "  # if selecting smote trained model\n",
        "  if smote == True:\n",
        "    hold_preds = model_smote.predict(X_data_scale_fs)\n",
        "\n",
        "  # else non-smote trained model\n",
        "  else:\n",
        "    hold_preds = knn_clf.predict(X_data_scale_fs)\n",
        "\n",
        "  # take max of predictions \n",
        "  #max_predictions = np.argmax(hold_preds, axis=1)\n",
        "\n",
        "  # metrics \n",
        "  print(\"\\n---------------------- Metrics ----------------------------------------\")\n",
        "\n",
        "  print(\"Accuracy : \\t\\t\" ,accuracy_score(y_data, hold_preds))\n",
        "  print(\"Balanced Accuracy : \\t\" , balanced_accuracy_score(y_data, hold_preds))\n",
        "  #print(\"F1 Score : \\t\\t\" , f1_score(y_data, hold_preds, average='weighted'))\n",
        "\n",
        "\n",
        "  # set up labels \n",
        "  LABELS = ['Go', 'Turn1',  'Turn2' , 'Walk1', 'Walk2', 'Sit']\n",
        "\n",
        "  # classification report \n",
        "  print(\"\\n------------------- HoldOut Classification Report ---------------\")\n",
        "  print(classification_report(y_data , hold_preds))\n",
        "  print(\" \")\n",
        "\n",
        "  # confusion matrix\n",
        "  confusion_matrix_out = metrics.confusion_matrix(y_data, hold_preds )\n",
        "\n",
        "  plt.figure(figsize=(14, 10))\n",
        "  sns.heatmap(confusion_matrix_out, xticklabels=LABELS, yticklabels=LABELS, annot=True ,fmt=\"d\" );\n",
        "  plt.title(\"HoldOut Data Confusion matrix\")\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "  # lastly just printing actual predictions\n",
        "  print(\"\\n0 = Go \\t\\t 1 = Turn 1 \\t 2 = Turn 2\")\n",
        "  print(\"\\n3 = Walk 1 \\t 4 = Walk 2 \\t 5 = Sit\")\n",
        "\n",
        "  print(\" \")\n",
        "  print(hold_preds)\n",
        "\n",
        "  print(Counter(hold_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvrs6vgM7f2z"
      },
      "source": [
        "holdout_checker_knn()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}