{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_SMOTE_Manual_Train_Test_HandCraft_Thesis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx-oyHiO2QXg"
      },
      "source": [
        "------------------------------\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "- Seemingly no difference between SMOTE VS No SMOTE\n",
        "\n",
        "- Hyper Tuning of MLP , Bagging , Decision Tree\n",
        "\n",
        "\n",
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrVWaVNkPW2k"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDjv2eMjASXA"
      },
      "source": [
        "#!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtX5iMQHPDII"
      },
      "source": [
        "\n",
        "########################### (https://github.com/curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs/blob/master/human_activity_recognition.ipynb) imports\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score , f1_score, recall_score, precision_score\n",
        "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_M5F5DwPrKk"
      },
      "source": [
        "# Upload Data \n",
        "\n",
        "---------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZX9OGsWPzvD"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IyDYp-eLY8K"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_train = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXPVt1AP0v1"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDlYInx_Pyx5"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_signal_test = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIL8q5nvqUf"
      },
      "source": [
        "### Reading in segments & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y70o5U5mSOUU"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxXU40eBttE6"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_dataframes_train = [key for key in uploaded_signal_train.keys()]\n",
        "\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_dataframe_train = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_dataframes_train)):\n",
        "\n",
        "    # load in the data \n",
        "    dataframe_train = pd.read_csv(list_of_dataframes_train[i])\n",
        "\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_dataframe_train.append(dataframe_train) \n",
        "\n",
        "\n",
        "all_df_train = pd.concat(all_dataframe_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXzYX7vVSPqK"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s9Wo8YGSQeN"
      },
      "source": [
        "# getting keys, which is file names of npy \n",
        "list_of_dataframes_test = [key for key in uploaded_signal_test.keys()]\n",
        "\n",
        "\n",
        "# set up list to hold all loaded npy \n",
        "all_dataframe_test = [] \n",
        "\n",
        "\n",
        "for i in range(len(list_of_dataframes_test)):\n",
        "\n",
        "    # load in the data \n",
        "    dataframe_test = pd.read_csv(list_of_dataframes_test[i])\n",
        "\n",
        "\n",
        "    # append the data to 'all' list\n",
        "    all_dataframe_test.append(dataframe_test) \n",
        "\n",
        "\n",
        "all_df_test = pd.concat(all_dataframe_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jsSnExgGesF"
      },
      "source": [
        "# Quick Look "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCEZ0lTrGiz3"
      },
      "source": [
        "all_df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ-KTwnDGhYv"
      },
      "source": [
        "all_df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVPv0iQ6SPeH"
      },
      "source": [
        "# Train Test Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlyxtKg4SRMP"
      },
      "source": [
        "# Getting X_train & y_train'\n",
        "X_train = all_df_train.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run', 'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'\t], axis = 1)\n",
        "y_train = all_df_train['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyOL-dTlTDbw"
      },
      "source": [
        "# Getting X_train & y_train\n",
        "X_test = all_df_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run',  'X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands'], axis = 1)\n",
        "y_test = all_df_test['Label_segment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vM6doFdbvla"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tVnwD0csNgw"
      },
      "source": [
        "Standard Scale "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFhyhbcjsO7K"
      },
      "source": [
        "# Set up ss for non-shuffled data\n",
        "ss = StandardScaler()\n",
        "\n",
        "\n",
        "# fit train & transform test\n",
        "X_train_scale = ss.fit_transform(X_train)\n",
        "X_test_scale = ss.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpApWRNA4f0"
      },
      "source": [
        "-----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY2CvefFA3Pd"
      },
      "source": [
        "Check if SMOTE helps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivu4MogoA3Pe"
      },
      "source": [
        "print(Counter(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iIID62gA3Pf"
      },
      "source": [
        "# transform the dataset\n",
        "oversample = SMOTE()\n",
        "X_train_scale_smote, y_train_smote = oversample.fit_resample(X_train_scale, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzRxVv6A3Ph"
      },
      "source": [
        "print(Counter(y_train_smote))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZSss6u0jQN_"
      },
      "source": [
        "----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgzUsp7ZvsfE"
      },
      "source": [
        "# BaseLine - No SMOTE No Feature Reduction  \n",
        "\n",
        "Check Some Classifiers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu9bDnSz3bZl"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss , recall_score , f1_score, precision_score , roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "classifiers = [\n",
        "               \n",
        "    # Too much computation to run now since using entire data and not segmenting on location \n",
        "    ##KNeighborsClassifier(),\n",
        "    ##GradientBoostingClassifier(),\n",
        "    ##LabelPropagation(),\n",
        "\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "    LinearSVC(),\n",
        "    SGDClassifier(),\n",
        "    MLPClassifier(),\n",
        "    PassiveAggressiveClassifier(),\n",
        "    ExtraTreesClassifier(),\n",
        "    BaggingClassifier(),\n",
        "  \n",
        "    \n",
        "     ]\n",
        "\n",
        "\n",
        "# list to hold for dataframe\n",
        "classifier_name_list = []\n",
        "train_acc_list = []\n",
        "train_bacc_list = []\n",
        "test_acc_list = []\n",
        "test_bacc_list = []\n",
        "train_recall_list = []\n",
        "test_recall_list = []\n",
        "train_precision_list = []\n",
        "test_precision_list = []\n",
        "train_f1_list = []\n",
        "test_f1_list = []\n",
        "\n",
        "\n",
        "for clf in classifiers:\n",
        "    \n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    print(name)\n",
        "\n",
        "    clf.fit(X_train_scale, y_train)\n",
        "    \n",
        "    print('****Results****')\n",
        "\n",
        "    # Test Predictions\n",
        "    test_predictions = clf.predict(X_test_scale)\n",
        "\n",
        "    # Test Metrics\n",
        "    acc            = accuracy_score(y_test, test_predictions)\n",
        "    bal_acc        = balanced_accuracy_score(y_test, test_predictions)\n",
        "    recall_test    = recall_score(y_test, test_predictions, average = 'weighted')\n",
        "    f1_test        = f1_score(y_test, test_predictions ,  average = 'weighted')\n",
        "    precision_test = precision_score(y_test, test_predictions,  average = 'weighted') \n",
        "\n",
        "\n",
        "\n",
        "    # Train Predictions\n",
        "    train_predictions = clf.predict(X_train_scale)\n",
        "\n",
        "    # Train Metrics\n",
        "    train_acc       = accuracy_score(y_train, train_predictions)\n",
        "    train_bal_acc   = balanced_accuracy_score(y_train, train_predictions)\n",
        "    recall_train    = recall_score(y_train, train_predictions , average = 'weighted')\n",
        "    f1_train        = f1_score(y_train, train_predictions ,  average = 'weighted')\n",
        "    precision_train = precision_score(y_train, train_predictions,  average = 'weighted') \n",
        "\n",
        "    print(\"\\n\\nTest Classification Report\\n\")\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # append to list to make a dataframe \n",
        "    classifier_name_list.append(name)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(acc)\n",
        "\n",
        "    train_bacc_list.append(train_bal_acc)\n",
        "    test_bacc_list.append(bal_acc)\n",
        "\n",
        "    train_recall_list.append(recall_train)\n",
        "    test_recall_list.append(recall_test)\n",
        "\n",
        "    train_precision_list.append(precision_train)\n",
        "    test_precision_list.append(precision_test)\n",
        "\n",
        "    train_f1_list.append(f1_train)\n",
        "    test_f1_list.append(f1_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbVPvPjixeRh"
      },
      "source": [
        "train_metrics_df = pd.DataFrame()\n",
        "test_metrics_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "train_metrics_df['Classifier'] = classifier_name_list\n",
        "test_metrics_df['Classifier'] = classifier_name_list\n",
        "\n",
        "# F1 Score First \n",
        "train_metrics_df['Train F1'] = train_f1_list\n",
        "test_metrics_df['Test F1'] = test_f1_list \n",
        "\n",
        "# Recall\n",
        "train_metrics_df['Train Recall'] = train_recall_list\n",
        "test_metrics_df['Test Recall'] = test_recall_list\n",
        "\n",
        "# Precision \n",
        "train_metrics_df['Train Precision'] = train_precision_list\n",
        "test_metrics_df['Test Precision'] = test_precision_list\n",
        "\n",
        "# Bal Acc\n",
        "train_metrics_df['Train Balanced Accuracy'] = train_bacc_list\n",
        "test_metrics_df['Test Balanced Accuracy'] = test_bacc_list\n",
        "\n",
        "# Accuracy \n",
        "train_metrics_df['Train Accuracy'] = train_acc_list\n",
        "test_metrics_df['Test Accuracy'] = test_acc_list \n",
        "\n",
        "\n",
        "train_metrics_df.sort_values(\"Train F1\" , ascending=False , inplace=True)\n",
        "train_metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "test_metrics_df.sort_values(\"Test F1\" , ascending=False , inplace=True)\n",
        "test_metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "# display df\n",
        "display(train_metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agmoQlcKoG34"
      },
      "source": [
        "# display df\n",
        "display(test_metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rcBBSUHvqYv"
      },
      "source": [
        "---------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0WIFmVLCJnv"
      },
      "source": [
        "# SMOTE \n",
        "\n",
        "Check Some Classifiers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R4NDyQhGgvA"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss , recall_score , f1_score, precision_score , roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "classifiers = [\n",
        "               \n",
        "    # Too much computation to run now since using entire data and not segmenting on location \n",
        "    ##KNeighborsClassifier(),\n",
        "    ##GradientBoostingClassifier(),\n",
        "    ##LabelPropagation(),\n",
        "\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "    LinearSVC(),\n",
        "    SGDClassifier(),\n",
        "    MLPClassifier(),\n",
        "    PassiveAggressiveClassifier(),\n",
        "    ExtraTreesClassifier(),\n",
        "    BaggingClassifier(),\n",
        "  \n",
        "    \n",
        "     ]\n",
        "\n",
        "\n",
        "# list to hold for dataframe\n",
        "classifier_name_list = []\n",
        "train_acc_list = []\n",
        "train_bacc_list = []\n",
        "test_acc_list = []\n",
        "test_bacc_list = []\n",
        "train_recall_list = []\n",
        "test_recall_list = []\n",
        "train_precision_list = []\n",
        "test_precision_list = []\n",
        "train_f1_list = []\n",
        "test_f1_list = []\n",
        "\n",
        "\n",
        "for clf in classifiers:\n",
        "    \n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*50)\n",
        "    print(name)\n",
        "\n",
        "    clf.fit(X_train_scale, y_train)\n",
        "    \n",
        "    print('****Results****')\n",
        "\n",
        "    # Test Predictions\n",
        "    test_predictions = clf.predict(X_test_scale)\n",
        "\n",
        "    # Test Metrics\n",
        "    acc            = accuracy_score(y_test, test_predictions)\n",
        "    bal_acc        = balanced_accuracy_score(y_test, test_predictions)\n",
        "    recall_test    = recall_score(y_test, test_predictions, average = 'weighted')\n",
        "    f1_test        = f1_score(y_test, test_predictions ,  average = 'weighted')\n",
        "    precision_test = precision_score(y_test, test_predictions,  average = 'weighted') \n",
        "\n",
        "\n",
        "\n",
        "    # Train Predictions\n",
        "    train_predictions = clf.predict(X_train_scale)\n",
        "\n",
        "    # Train Metrics\n",
        "    train_acc       = accuracy_score(y_train, train_predictions)\n",
        "    train_bal_acc   = balanced_accuracy_score(y_train, train_predictions)\n",
        "    recall_train    = recall_score(y_train, train_predictions , average = 'weighted')\n",
        "    f1_train        = f1_score(y_train, train_predictions ,  average = 'weighted')\n",
        "    precision_train = precision_score(y_train, train_predictions,  average = 'weighted') \n",
        "\n",
        "    print(\"\\n\\nTest Classification Report\\n\")\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # append to list to make a dataframe \n",
        "    classifier_name_list.append(name)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(acc)\n",
        "\n",
        "    train_bacc_list.append(train_bal_acc)\n",
        "    test_bacc_list.append(bal_acc)\n",
        "\n",
        "    train_recall_list.append(recall_train)\n",
        "    test_recall_list.append(recall_test)\n",
        "\n",
        "    train_precision_list.append(precision_train)\n",
        "    test_precision_list.append(precision_test)\n",
        "\n",
        "    train_f1_list.append(f1_train)\n",
        "    test_f1_list.append(f1_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4etuIVvbyA54"
      },
      "source": [
        "train_metrics_df = pd.DataFrame()\n",
        "test_metrics_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "train_metrics_df['Classifier'] = classifier_name_list\n",
        "test_metrics_df['Classifier'] = classifier_name_list\n",
        "\n",
        "# F1 Score First \n",
        "train_metrics_df['Train F1'] = train_f1_list\n",
        "test_metrics_df['Test F1'] = test_f1_list \n",
        "\n",
        "# Recall\n",
        "train_metrics_df['Train Recall'] = train_recall_list\n",
        "test_metrics_df['Test Recall'] = test_recall_list\n",
        "\n",
        "# Precision \n",
        "train_metrics_df['Train Precision'] = train_precision_list\n",
        "test_metrics_df['Test Precision'] = test_precision_list\n",
        "\n",
        "# Bal Acc\n",
        "train_metrics_df['Train Balanced Accuracy'] = train_bacc_list\n",
        "test_metrics_df['Test Balanced Accuracy'] = test_bacc_list\n",
        "\n",
        "# Accuracy \n",
        "train_metrics_df['Train Accuracy'] = train_acc_list\n",
        "test_metrics_df['Test Accuracy'] = test_acc_list \n",
        "\n",
        "\n",
        "train_metrics_df.sort_values(\"Train F1\" , ascending=False , inplace=True)\n",
        "train_metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "test_metrics_df.sort_values(\"Test F1\" , ascending=False , inplace=True)\n",
        "test_metrics_df.set_index('Classifier' , inplace=True)\n",
        "\n",
        "# display df\n",
        "display(train_metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6F6cMhKLblk"
      },
      "source": [
        "# display df\n",
        "display(test_metrics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGs_oxsnGhkP"
      },
      "source": [
        "---------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D56bWRmC5_ik"
      },
      "source": [
        "mlp_clf = MLPClassifier(early_stopping=True, alpha=5 )\n",
        "\n",
        "mlp_clf.fit(X_train_scale , y_train)\n",
        "\n",
        "train_preds = mlp_clf.predict(X_train_scale)\n",
        "test_preds  = mlp_clf.predict(X_test_scale)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTRIP8kf6x2j"
      },
      "source": [
        "print(classification_report(y_train, train_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmnotQjf6tR3"
      },
      "source": [
        "print(classification_report(y_test, test_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s9dTXk87zOv"
      },
      "source": [
        "mlp_clf_smote = MLPClassifier(early_stopping=True, alpha= 5)\n",
        "\n",
        "mlp_clf_smote.fit(X_train_scale_smote , y_train_smote)\n",
        "\n",
        "train_preds_smote = mlp_clf_smote.predict(X_train_scale_smote)\n",
        "test_preds_smote  = mlp_clf_smote.predict(X_test_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZxbmpJC75vL"
      },
      "source": [
        "print(classification_report(y_train_smote, train_preds_smote))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHll0UbL7_Su"
      },
      "source": [
        "print(classification_report(y_test, test_preds_smote))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6JvdQ7u7l0E"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbhYn60H7mfA"
      },
      "source": [
        "def holdout_checker_mlp(smote = False ):\n",
        "\n",
        "  # Read in Fresh Data to see what predictions looks like\n",
        "  from google.colab import files\n",
        "  uploaded_test = files.upload()\n",
        "\n",
        "  # getting keys, which is file names of csv\n",
        "  val_file_name = [key for key in uploaded_test.keys()]\n",
        "\n",
        "  # read in csv \n",
        "  signal_test = pd.read_csv(val_file_name[0])\n",
        "\n",
        "\n",
        "  # Getting X_train & y_train\n",
        "  X_data = signal_test.drop(['Unnamed: 0' , 'Label_segment' , 'Participant_ID' , 'Participant_Run','X_Acc_Move_FFT_EnergyBands', \t'Y_Acc_Move_FFT_EnergyBands', \t'Z_Acc_Move_FFT_EnergyBands' ], axis = 1)\n",
        "  y_data = signal_test['Label_segment'].values\n",
        "\n",
        "  # scale with already fit ss \n",
        "  X_data_scale = ss.transform(X_data)\n",
        "\n",
        "  # feature selection\n",
        "  #X_data_scale_fs = fs.transform(X_data_scale)\n",
        "\n",
        "\n",
        "  # if selecting smote trained model\n",
        "  if smote == True:\n",
        "    hold_preds = mlp_clf_smote.predict(X_data_scale)\n",
        "\n",
        "  # else non-smote trained model\n",
        "  else:\n",
        "    hold_preds = mlp_clf.predict(X_data_scale)\n",
        "\n",
        "  # take max of predictions \n",
        "  #max_predictions = np.argmax(hold_preds, axis=1)\n",
        "\n",
        "  # metrics \n",
        "  print(\"\\n---------------------- Metrics ----------------------------------------\")\n",
        "\n",
        "  print(\"Accuracy : \\t\\t\" ,accuracy_score(y_data, hold_preds))\n",
        "  print(\"Balanced Accuracy : \\t\" , balanced_accuracy_score(y_data, hold_preds))\n",
        "  #print(\"F1 Score : \\t\\t\" , f1_score(y_data, hold_preds, average='weighted'))\n",
        "\n",
        "\n",
        "  # set up labels \n",
        "  LABELS = ['Go', 'Turn1',  'Turn2' , 'Walk1', 'Walk2', 'Sit']\n",
        "\n",
        "  # classification report \n",
        "  print(\"\\n------------------- HoldOut Classification Report ---------------\")\n",
        "  print(classification_report(y_data , hold_preds))\n",
        "  print(\" \")\n",
        "\n",
        "  # confusion matrix\n",
        "  confusion_matrix_out = metrics.confusion_matrix(y_data, hold_preds )\n",
        "\n",
        "  plt.figure(figsize=(14, 10))\n",
        "  sns.heatmap(confusion_matrix_out, xticklabels=LABELS, yticklabels=LABELS, annot=True ,fmt=\"d\" );\n",
        "  plt.title(\"HoldOut Data Confusion matrix\")\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "  # lastly just printing actual predictions\n",
        "  print(\"\\n0 = Go \\t\\t 1 = Turn 1 \\t 2 = Turn 2\")\n",
        "  print(\"\\n3 = Walk 1 \\t 4 = Walk 2 \\t 5 = Sit\")\n",
        "\n",
        "  print(\" \")\n",
        "  print(hold_preds)\n",
        "\n",
        "  print(Counter(hold_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq0RzkPW8dGb"
      },
      "source": [
        "holdout_checker_mlp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMJjgO6B8itt"
      },
      "source": [
        "holdout_checker_mlp(smote=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPX7LUOmvFb"
      },
      "source": [
        "------------------------------------------\n",
        "\n",
        "MLP Tune "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs6Lu0l2mxpZ"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIitBXRNm5Gi"
      },
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [100,200,500,1000],\n",
        "    'activation': ['tanh', 'relu', 'logistic'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': np.arange(0, 0.9, 0.1),\n",
        "    'learning_rate': ['constant','adaptive']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hIz6ZzwnJG6"
      },
      "source": [
        "gsearch = RandomizedSearchCV(estimator = MLPClassifier(),\n",
        "                        param_distributions = parameter_space,  \n",
        "                        scoring = 'balanced_accuracy',                \n",
        "                        cv = 2,\n",
        "                        n_jobs = -1,\n",
        "                        verbose = 4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsUET1YjnR9v"
      },
      "source": [
        "grid_search = gsearch.fit(X_train_scale, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG7jnPC3nwMg"
      },
      "source": [
        "gs_model = grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUvnmAQsnwuP"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhardnc5oRMZ"
      },
      "source": [
        "preds = gs_model.predict(X_test_scale)\n",
        "\n",
        "\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA6t9_wtlv8_"
      },
      "source": [
        "print(\"\\n\\nTest Classification Report\\n\")\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk-6w70QZ0Qt"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCApYDa4pj4k"
      },
      "source": [
        "-----------------------------------------------\n",
        "\n",
        "Tuned Decision Tree Classifier \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMosygwoMjJo"
      },
      "source": [
        "params = {\n",
        "    'max_depth': [ 20, 30,50, 100, 1000],\n",
        "    'min_samples_leaf': [20, 50, 100, 500, 1000, 5000, 10000],\n",
        "    'criterion': [\"gini\", \"entropy\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_wCjh5xMrk0"
      },
      "source": [
        "gsearch_2 = RandomizedSearchCV(estimator = DecisionTreeClassifier(),\n",
        "                        param_distributions = params,  \n",
        "                        scoring = 'balanced_accuracy',                \n",
        "                        cv = 3,\n",
        "                        n_jobs = -1,\n",
        "                        verbose = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LU5xG2tMzl0"
      },
      "source": [
        "grid_search_2 = gsearch_2.fit(X_train_scale, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJl2Ba5OZt3"
      },
      "source": [
        "gs_model_2 = grid_search_2.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-itU-CAOb-v"
      },
      "source": [
        "grid_search_2.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K98ZV7ifOfEh"
      },
      "source": [
        "preds = gs_model_2.predict(X_test_scale)\n",
        "\n",
        "train_preds = gs_model_2.predict(X_train_scale)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdjCP9zvW-Hc"
      },
      "source": [
        "print(\"Train Classification Report\\n\")\n",
        "print(classification_report(y_train, train_preds))\n",
        "\n",
        "print(\"\\n\\nTest Classification Report\\n\")\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYB98RrXaIgS"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ajUDXbZQ91"
      },
      "source": [
        "------------------------------------\n",
        "\n",
        "AdaBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiMg0m98ZTMz"
      },
      "source": [
        "ab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
        "\n",
        "parameters = {'base_estimator__max_depth':[i for i in range(2,11,3)],\n",
        "              'base_estimator__min_samples_leaf':[5,10],\n",
        "              'n_estimators':[10,50,250,1000],\n",
        "              'learning_rate':[0.01,0.1]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKc1isQGZW4O"
      },
      "source": [
        "gsearch_3 = RandomizedSearchCV(estimator = ab,\n",
        "                        param_distributions = parameters,  \n",
        "                        scoring = 'balanced_accuracy',                \n",
        "                        cv = 2,\n",
        "                        n_jobs = -1,\n",
        "                        verbose = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWsoLEF9Zf6G"
      },
      "source": [
        "grid_searc_3 = gsearch_3.fit(X_train_scale, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgHivdJEaM8h"
      },
      "source": [
        "gs_model_3 = grid_searc_3.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z645BYZaP_U"
      },
      "source": [
        "grid_searc_3.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8GrQltzaRTu"
      },
      "source": [
        "preds = gs_model_3.predict(X_test_scale)\n",
        "\n",
        "train_preds = gs_model_3.predict(X_train_scale)\n",
        "\n",
        "print(\"-------------------Train------------------\")\n",
        "print(\"Train Balanced Accuracy : \\t\" , balanced_accuracy_score(y_train, train_preds))\n",
        "print(\"Train F1 Score : \\t\\t\" , f1_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Precision Score : \\t\" , precision_score(y_train, train_preds , average='weighted'))\n",
        "print(\"Train Recall Score : \\t\\t\" , recall_score(y_train, train_preds , average='weighted'))\n",
        "\n",
        "print(\"\\n\\n-------------------Test------------------\")\n",
        "print(\"Test Balanced Accuracy : \\t\" , balanced_accuracy_score(y_test, preds))\n",
        "print(\"Test F1 Score : \\t\\t\" , f1_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Precision Score : \\t\\t\" , precision_score(y_test, preds , average='weighted'))\n",
        "print(\"Test Recall Score : \\t\\t\" , recall_score(y_test, preds , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}